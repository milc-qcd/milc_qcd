\input texinfo.tex @c -*- Mode: Texinfo -*-
@headings doubleafter
@setfilename milcv7.info
@settitle The MILC Code (version: 7.7.11)
@footnotestyle end
@c @setchapternewpage off

@ignore
 c milcv7.texi -- texinfo source for MILC code documentation
 c Copyright (C) 2011 The MILC Collaboration
 c Contact: <detar@physics.utah.edu> or <doug@physics.arizona.edu>
 c
 c  to compile: texi2dvi milcv7.texi, texi2html milcv7.texi
 c $Id: milcv7.texi,v 1.9 2013/12/26 15:57:20 detar Exp $
 c
 c $Log: milcv7.texi,v $
 c Revision 1.9  2013/12/26 15:57:20  detar
 c Updates for 7.7.10
 c
 c Revision 1.8  2012/04/25 03:43:51  detar
 c Correct wprop->ksprop typo
 c
 c Revision 1.7  2012/03/06 03:27:35  detar
 c Explain "sourceonly" and rule for creating a writeable source
 c
 c Revision 1.6  2012/02/22 05:37:24  detar
 c Updates
 c
 c Revision 1.5  2012/01/21 21:19:34  detar
 c Correct contributor list.
 c
 c Revision 1.4  2011/12/03 15:04:42  detar
 c Add pdf capability.  Minor edits.
 c
 c Revision 1.3  2011/12/03 14:52:29  detar
 c Update documentation for version 7.7.3
 c
 c Revision 1.2  2011/11/30 03:18:38  detar
 c First draft of 7.7.2 documentation
 c
 c Revision 1.1  2008/03/28 15:44:32  detar
 c Upgrade documentation
 c
 c Revision 1.1.1.1  2005/02/23 00:04:49  detar
 c Imported sources
 c
@end ignore

@ifinfo
@node Top, Obtaining the MILC Code, (dir), (dir)
@top The MILC Code
@center The MILC Code 
@center version 7.7.11
@sp 1
@center @strong{Alexei Bazavov} (@emph{Brookhaven National Laboratory}) <obazavov@@quark.phy.bnl.gov>
@center @strong{Claude Bernard} (@emph{Washington U.}) <cb@@lump.wustl.edu>
@center @strong{Nathan Brown} (@emph{Washington U.}) <zen.oehler@gmail.com>
@center @strong{Tom Burch} (@emph{U. of Utah}) <tburch@@physics.utah.edu>
@center @strong{Tom DeGrand} (@emph{U. of Colorado}) <degrand@@aurinko.colorado.edu>
@center @strong{Carleton DeTar} (@emph{U. of Utah}) <detar@@physics.utah.edu>
@center @strong{Justin Foley} (@emph{U. of Utah}) <jfoley@@physics.utah.edu>
@center @strong{Steve Gottlieb} (@emph{Indiana U.}) <sg@@denali.physics.indiana.edu>
@center @strong{Urs Heller} (@emph{APS}) <heller@@ridge.aps.org,>
@center @strong{James Hetrick} (@emph{U. of the Pacific}) <jhetrick@@uop.edu>
@center @strong{Ludmila Levkova} (@emph{U. of Utah}) <ludmila@@physics.utah.edu>
@center @strong{Craig McNeile} (@emph{Glasgow U.}) <c.mcneile@@physics.gla.ac.uk>
@center @strong{Kostas Orginos} (@emph{College of William and Mary}) <kostas@@kostas@@jlab.org>
@center @strong{James Osborn} (@emph{Argonne National Laboratory}) <osborn@@alcf.anl.gov>
@center @strong{Kari Rummukainen} (@emph{Oulu University}) <kari.rummukainen@@oulu.fi>
@center @strong{Bob Sugar} (@emph{U.C. Santa Barbara}) <sugar@@sarek.physics.ucsb.edu>
@center @strong{Doug Toussaint} (@emph{U. of Arizona}) <doug@@klingon.physics.arizona.edu>

The MILC Code is a body of high performance research software written
in C for doing SU(3) lattice gauge theory on several different (MIMD)
parallel computers in current use. In scalar mode, it runs on a
variety of workstations making it extremely versatile for both
production and exploratory applications. This manual is for the latest
(7.7.11) version of the code. Currently supported code runs
on:
@itemize @bullet
@item Scalar machines
@item Linux+MPI clusters
@item IBM BG/L BG/P BG/Q
@item Cray XT3, XE6
@item Multi-GPU clusters
@end itemize
This is a @TeX{}info document; an HTML version is accessible at:
@itemize 
@item http://www.physics.utah.edu/~detar/milc/
@end itemize


@ignore
Permission is granted to process this file through TeX and print the
results, provided the printed document carries a copying permission
notice identical to this one except for the removal of this paragraph
(this paragraph not being relevant to the printed manual).
@end ignore
@end ifinfo

@node Top, Obtaining the MILC Code, (dir), (dir)
@top The MILC Code
@titlepage
@center @titlefont{The MILC Code}
@sp 1

@center version ---7.7.11---
@sp 1
@center @strong{Alexei Bazavov} (@emph{Brookhaven National Laboratory}) <obazavov@@quark.phy.bnl.gov>
@center @strong{Claude Bernard} (@emph{Washington U.}) <cb@@lump.wustl.edu>
@center @strong{Tom Burch} (@emph{U. of Utah}) <tburch@@physics.utah.edu>
@center @strong{Tom DeGrand} (@emph{U. of Colorado}) <degrand@@aurinko.colorado.edu>
@center @strong{Carleton DeTar} (@emph{U. of Utah}) <detar@@physics.utah.edu>
@center @strong{Justin Foley} (@emph{U. of Utah}) <jfoley@@physics.utah.edu>
@center @strong{Steve Gottlieb} (@emph{Indiana U.}) <sg@@denali.physics.indiana.edu>
@center @strong{Urs Heller} (@emph{APS}) <heller@@ridge.aps.org,>
@center @strong{James Hetrick} (@emph{U. of the Pacific}) <jhetrick@@uop.edu>
@center @strong{Ludmila Levkova} (@emph{U. of Utah}) <ludmila@@physics.utah.edu>
@center @strong{Craig McNeile} (@emph{Glasgow U.}) <c.mcneile@@physics.gla.ac.uk>
@center @strong{Kostas Orginos} (@emph{College of William and Mary}) <kostas@@kostas@@jlab.org>
@center @strong{James Osborn} (@emph{Argonne National Laboratory}) <osborn@@alcf.anl.gov>
@center @strong{Kari Rummukainen} (@emph{Oulu University}) <kari.rummukainen@@oulu.fi>
@center @strong{Bob Sugar} (@emph{U.C. Santa Barbara}) <sugar@@sarek.physics.ucsb.edu>
@center @strong{Doug Toussaint} (@emph{U. of Arizona}) <doug@@klingon.physics.arizona.edu>

The MILC Code is a body of high performance research software written
in C for doing SU(3) lattice gauge theory on several different (MIMD)
parallel computers in current use. In scalar mode, it runs on a
variety of workstations making it extremely versatile for both
production and exploratory applications. This manual is for the latest
(7.7.11) version of the code. Currently supported code runs
on:
@itemize @bullet
@item Scalar machines
@item Linux+MPI clusters
@item IBM BG/L BG/P BG/Q
@item Cray XT3, XE6
@item Multi-GPU clusters
@end itemize
This is a @TeX{}info document; an HTML version is accessible at:
@itemize 
@item http://www.physics.utah.edu/~detar/milc/
@end itemize
@ifinfo
This is a @TeX{}info document; an HTML version is accessible at:
@end ifinfo
@itemize
@item http://www.physics.utah.edu/~detar/milc/
@end itemize
At present there is no special accommodation for parallel architectures
with multiple share-memory processors (SMP) on a node.  Each processor
is treated as though it is a separate node, requiring a communications
operation to exchange data, regardless of whether data is in commonly
shared memory or truly off node.  Throughout this documentation ``node''
is therefore synonymous with ``processor.''

@page
@vskip 0pt plus 1filll
@cindex Copyright
Copyright @copyright{} 2011 by The MILC Collaboration

Permission is granted to make and distribute verbatim copies of this
manual provided the copyright notice and this permission notice are
preserved on all copies.

@cindex Last change
@c ************************** LAST CHANGE *****************************
@*Last change: [detar:06 15 2011]
@c ********************************************************************
@end titlepage

@menu
* Obtaining the MILC Code::
* Building the MILC Code::
* Command line options::
* General description::
* Programming with MILC Code::
* Writing Your Own Application::
* Documentation for Specific Applications::
* Concept Index::
* Variable Index::
@end menu

@node Obtaining the MILC Code, Building the MILC Code, Top, Top
@cindex Obtaining the MILC Code
@chapter Obtaining the MILC Code

This chapter explains how to get the code, copyright conditions and the
installation process.

@menu
* Web sites::
* Usage conditions::
* Installing the MILC Code::
* Portability::
* SciDAC Support::
* GPU Support::
* Supported architectures::
@end menu

@node Web sites, Usage conditions, Obtaining the MILC Code, Obtaining the MILC Code
@section Web sites
@cindex  Web sites
@cindex MILC Homepage

The most up-to-date information and access to the MILC Code can be found

@itemize @bullet  
@item via WWW at: 
@itemize
@item @kbd{http://physics.utah.edu/~detar/milc/}
@end itemize

@item via email request to the authors' representatives at: 
@itemize
@item @kbd{doug@@klingon.physics.arizona.edu}
@item @kbd{detar@@physics.utah.edu}
@end itemize
@end itemize

@node Usage conditions, Installing the MILC Code, Web sites, Obtaining the MILC Code
@section Usage conditions
@cindex Usage conditions
@cindex Free Software Foundation
@cindex GNU General Public License

The MILC Code is free software; you can redistribute it and/or modify it
under the terms of the GNU General Public License as published by the
Free Software Foundation.

@strong{Publications of research work done using this code or
derivatives of this code should acknowledge its use.} The MILC project
is supported in part by grants from the US Department of Energy and
National Science Foundation, and we ask that you use (at least) the
following string in publications which derive results using this
material: 
@sp 1

@dfn{This work was in part based on the MILC
collaboration's public lattice gauge theory code. See
@strong{http://physics.utah.edu/~detar/milc.html}}
@sp 1

This software is distributed in the hope that it will be useful, but
without any warranty; without even the implied warranty of
merchantability or fitness for a particular purpose. See the GNU
General Public License for more details, a copy of which License can be
obtained from
@example
Free Software Foundation, Inc., 
675 Mass Ave, Cambridge, MA 02139, USA.
@end example

Permission is granted to copy and distribute modified versions of this
manual under the conditions for verbatim copying, provided that the
entire resulting derived work is distributed under the terms of a
permission notice identical to this one.

@node Installing the MILC Code, Portability, Usage conditions, Obtaining the MILC Code
@section Installing the MILC Code
@cindex Installing the MILC Code

Unpack with the command
@example
    tar -xzf milc_qcd*.tar.gz
@end example
The procedure for building the MILC code is explained later in this
document (@pxref{Building the MILC Code}) or in the @kbd{README} file
that accompanies the code.

@node Portability, SciDAC Support, Installing the MILC Code, Obtaining the MILC Code
@section Portability
@cindex Portability
@cindex Installing the Code

One of our aims in writing this code was to make it very portable
across machine architectures and configurations. While the code must
be compiled with the architecture-specific low-level communication
files (@pxref{Building the MILC Code}), the application codes contain
a minimum of architecture-dependent @strong{#ifdef}'s, which now are
mostly for machine-dependent performance optimizations in the
conjugate gradient routines, etc, since MPI has become a portable
standard.

Similarly, with regard to random numbers, care has been taken to ensure
convenience and reproducibility. With @strong{SITERAND} set
(@pxref{Random numbers}), the random number generator will produce
the same sequence for a given seed, indendent of architecture and
the number of nodes.

@node SciDAC Support, GPU Support, Portability, Obtaining the MILC Code
@section SciDAC Support
@cindex SciDAC Support

The software component of the U.S. Department of Energy Lattice QCD
SciDAC project provides a multilayer interface for lattice gauge
theory code. It is intended to be portable even to specialized
platforms, such as GPU clusters.  This release of the MILC code
supports a wide variety of C-language SciDAC modules.  They are
invoked through compilation macros described below (@pxref{Optional
Compilation Macros}).

@node GPU Support, Supported architectures, SciDAC Support, Obtaining the MILC Code
@section GPU Support
@cindex GPU Support

Development-grade QUDA-based GPU support for staggered and HISQ
molecular dynamics.  Currently, only single-GPU operation is
supported.  Multi-GPU is planned.

@node Supported architectures,, SciDAC Support,  Obtaining the MILC Code
@section Supported architectures
@cindex Supported architectures
This manual covers @strong{version 7.7.11} which is currently supposed to
run on: 
@sp 1
@itemize @bullet
@item Scalar machines
@item Linux+MPI clusters
@item IBM BG/L BG/P BG/Q
@item Cray XT3, XE6
@item Multi-GPU clusters
@end itemize

In addition it has run in the past on
@itemize @bullet
@item SGI Origin 2000
@item IBM SP
@item NT Clusters
@item Compaq Alpha Clusters
@item The Intel iPSC-860
@item Intel Paragon
@item PVM (version 3.2)
@item The Ncube 2
@item The Thinking Machines CM5
@item SGI/Cray T3E
@item Columbia/BNL QCDOC
@end itemize
and many other long gone (but not forgotten) computers.

@cindex bugs reports
@cindex questions to the authors
Since this is our working code, it in a continual state of
development.  We informally support the code as best we can by
answering questions and fixing bugs.  We will be very grateful for
reports of problems and suggestions for improvements, which may be
sent to
@example
doug@@klingon.physics.arizona.edu
detar@@physics.utah.edu
@end example

@node Building the MILC Code, Command line options, Obtaining the MILC Code, Top
@chapter Building the MILC Code
@cindex Building the code

@cindex Makefiles
Here are the steps required to build the code.  An explanation follows.

@enumerate
@item Select the application and target you wish to build
@item Edit the @file{Makefile} to select compilation options
@item Edit the @file{libraries/Make_vanilla} make file.
@item Edit the @file{include/config.h}. (Usually unnecessary.)
@item (Optional) Build and install the SciDAC packages.
@item (Optional) Build and install the FFTW packages.
@item (Optional) Build and install LAPACK.
@item (Optional) Build and install PRIMME.
@item (Optional) Build and install QUDA.
@item Run @strong{make} for the appropriate target
@end enumerate

@enumerate
@item Select the application and target you wish to build

The physics code is organized in various application directories.
Within each directory there are various compilation choices
corresponding to each @strong{make} target.  Further variations are
controlled by macro definitions in the @file{Makefile}.  So, for
example, the @strong{pure_gauge} application directory contains code
for the traditional single-plaquette action.  Choices for
@strong{make} targets include a hybrid Monte Carlo code and an
overrelaxed heatbath code.  Among variations in the @file{Makefile}
are single-processor or multi-processor operation and single or double
precision computation.

@item Edit the @file{Makefile} to select compilation options

A generic @file{Makefile} is found in the top-level directory.  Copy
it to the application subdirectory and edit it there.  Comments in the
file explain the choices.

The scalar workstation code is written in ANSI standard C99.  If your
compiler is not ANSI compliant, try using the Gnu C compiler gcc
instead.  The code can also be compiled under C++, but it uses no
exclusively C++ constructs.

@item Edit the @file{libraries/Make_vanilla} make file.

The MILC code contains a library of single-processor linear algebra
routines.  (No communication occurs in these routines.)  The library
is automatically built when you build the application target.
However, you need to set the compiler and compiler flags in the
library make file @file{libraries/Make_vanilla}.  It is a good idea to
verify that the compilation of the libaries is compatible with the
compilation of the application, for example, by using the same
underlying compiler for the libraries and the application code.

The library consists of two archived library files, each with a single
and double-precision version.  The libraries @file{complex.1.a} and
@file{complex.2.a} do single and double precision complex arithmetic
and the libraries @file{su3.1.a} and @file{su3.2.a} do a wide variety
of single and double precison matrix and vector arithmetic.

If you are cross-compiling, i.e. the working processors are of a
different architecture from the compilation processor, you may also
need to select the appropriate archiver @strong{ar}.

Many present-day processors have SSE capability offering faster
floating point arithmetic.  Most compilers now support them, so
additional effort to support them is not necessary.  Alternatives
include compiling assembly-coded alternatives to some of the most
heavily used library routines and, for gcc, using inlined assembly
instructions.  There is vestigial and not continually tested support
for these alternatives.

@item Edit the @file{include/config.h}.

At the moment we do not use autoconf/automake to get information about
the system environment.  This file deals with variations in the
operating system.  In most cases it doesn't need to be changed.

@item (Optional) Build and install the SciDAC packages.

If you wish to compile and link against the SciDAC packages, you must
first build and install them.  Note that for full functionality of the
code, you will need QMP and QIO.  The MILC code supports the
following packages:
@example
QMP (message passing)
QIO (file I/O)
QLA (linear algebra)
QDP/C (data parallel)
QOPQDP (optimized higher level code, such as inverters)
@end example
They are open-source and available from
@strong{http://usqcd.jlab.org/usqcd-software/}.

@item (Optional) Build and install the LAPACK package.

For the BlueGene some of the optimized code uses LAPACK.

@item (Optional) Build and install the FFTW package.

The convolution routines for source and sink operators use fast
Fourier transforms in the FFTW package.  A native MILC FFT is used if
the FFTW package is not selected in the top-level Makefile, so an
application that does not require a convolution can be built..  The
native version is inefficient, however.

@item (Optional) Build and install the PRIMME package.

The PRIMME preconditioned multimethod eigensolver by James R. McCombs
and Andreas Stathopoulos is recommended but not required for the
@strong{arb_overlap} application.

@item (Optional) Build and install the QUDA package.

The QUDA package, developed by the USQCD collaboration, provides
support for GPU computations.  It is obtained from the GIT repository
@strong{https://github.com/lattice/quda}.

@item Run @strong{make} for the appropriate target

The generic @file{Make_template} file in the application directory
lists a variety of targets followed by a double colon @kbd{::}.

@end enumerate

@section Making the Libraries
@cindex making the libraries
The libraries are built automatically when you make the application
target.  However, it is a good idea to verify that the compilation of
the libaries is compatible with the compilation of the application.
There are two libraries needed for SU(3) operations.  They come in
single and double precision versions, indicated by the suffix
@strong{1} and @strong{2}.
@itemize @bullet
@item @strong{complex.1.a} and @strong{complex.2.a}
contain routines for operations on complex
numbers. See @file{complex.h} for a summary.
@item @strong{su3.1.a} and @strong{su3.2.a} 
contain routines for operations on SU3 matrices,
three element complex vectors, and Dirac vectors (twelve element complex
vectors) among others.  See @file{su3.h} for a summary.
@end itemize

@section Checking the Build
@cindex checking the build

It is a good idea to run the compiled code with the sample input file
and compare the results with the sample output.  Of course, the most
straightforward way to do this is to run the test by hand.  Since
comparing a long list of numbers is tedious, each application has a
facility for testing the code and doing the comparison automatically.

You can test the code automatically in single-processor or
multiprocessor mode.  A make target is provided for running regression
tests.  If you use it, and you are testing in MPP mode, you must edit
the file @file{Make_test_template} in the top-level directory to
specify the appropriate job launch command (usually @file{mpirun}).
Comments in the file specify what needs to be changed.  As released,
the @file{Make_test_template} file is set for a single-processor test.

Sample input and output files are provided for most make targets.  For
each application directory, they are found in the subdirectory
@file{test}.  The file @file{checklist} contains a list of regression
tests and their variants.  A test is specified by the name of its
executable, its precision, and, if applicable, one of various input
cases.  For example, in the application directory
@file{ks_imp_dyn/test}, the @file{checklist} file starts with the line

@code{exec  su3_rmd                           1 - - WARMUPS RUNNING}

The name of the executable is @file{su3_rmd}, the precision is
@file{1} for single precision, and there is only one test case
(indicated by the second dash).  Corresponding to each regression test
is a set of files, namely, a sample input file, a sample output file,
and an error tolerance file.  For the above example, the files are
@file{su3_rmd.1.sample-in}, @file{su3_rmd.1.sample-out},
and @file{su3_rmd.1.errtol}, respectively.

If you want to run the test by hand, first compile the code with the
desired precision, then go to the @file{test} directory, and run the
executable with one of the appropriate sample input files.  For the
above example, the command would be

@code{../su3_rmd < su3_rmd.1.sample-in}.

Compare the output with the file @file{su3_rmd.1.sample-out}.  There
are inevitably slight differences in the results because of roundoff
error.  The @file{su3_rmd.1.errtol} specifies tolerances for these
differences where they matter.  Probably, you won't want to compare
every number, so using the scripts invoked by @file{make check} is
more convenient.

A make target is provided for running regression tests.  To run all
the regression tests listed in the @file{checklist} file, go to the
main application directory and do

@code{make check}.

It is a good idea to redirect standard output and error to a file as in 

@code{make check 2>&1 > maketest.log}

so the test results can be examined individually.

To run a test for a selected case in the @file{checklist} file, do

@code{make check "EXEC=su3_rmd" "CASE=-" "PREC=1"}

Omitting the @file{PREC} definition causes both precisions to be
tested (assuming they both appear in the @file{checklist} file).
Omitting both the @file{PREC} and the @file{CASE} definition causes
all cases and precisions for the given executable to be tested.
 
The automated test consists in most cases of generating a file
@file{test/su3_rmd.n.test-out} for @file{n} = 1 or 2 (single or double
precision) and comparing key sections of the file with the trusted
output @file{test/su3_rmc.n.sample-out}.  The comparison checks
line-by-line the difference of numerical values on a line.  The file
@file{su3_rmd.n.errtol} specifies the tolerated differences.  An error
message is written if a field is found to differ by more than the
preset tolerance.  A summary of out-of-tolerance differences is
written to @file{su3_rmd.n.test-diff}.  Differences may arise because
of round-off or, in molecular dynamics updates, a round-off-initiated
drift.

Some regression tests involve secondary output files with similar
@file{sample-out}, @file{test-out} and @file{errtol} file-name
extensions.


@node Command line options, General description, Building the MILC Code, Top
@chapter Command line options
@cindex command line options
@cindex qmp-geom
@cindex qmp-jobs
@cindex geom
@cindex jobs
@cindex stdin
@cindex stdout
@cindex stderr

All applications take the same set of command-line options.  The
format with SciDAC QMP compilation is

@example
   <executable> [-qmp-geom <int(4)>] [-qmp-jobs <int(1-4)>] 
         [ stdin_path [ stdout_path [stderr_path]]]
@end example
and without QMP compilation it is
@example
   <executable> [-geom <int(4)>] [-jobs <int(1-4)>] 
         [ stdin_path [ stdout_path [stderr_path]]]
@end example

Normally standard input, output, and error are obtaine through Unix
redirection with @strong{< stdin_path} and @strong{> stdout_path},
etc, where @strong{stdin_path} and @strong{stdout_path} are names of
files containing input and output.  However some versions of
multiprocessor job launching do not support such redirection.  An
alternative is provided, which allows naming the files explicitly on
the command line.  The first name is always standard input.  The
second, if specified, is standard output, etc.

@cindex mesh view

On a switch-based machine, usually the assignment of sublattices to
processors makes little difference to job performance, so the division
is done automatically by the code.  On a mesh network performance is
sometimes helped by controlling the subdivision.  The
@strong{qmp-geom} option or @strong{geom} option
@example
  -qmp-geom mx my mz mt
@end example
specifies the dimensions of a grid of processors.  The product must
equal the number of allocated processors and the grid dimensions must
separately be divisors of the corresponding lattice dimensions.  The
same effect is accomplished in some applications by the
@strong{node_geometry} parameter.  If both specifications are used,
the command-line option takes precedence.

@cindex multijob operation

In some operating environments it may be desirable to run multiple,
independent jobs on the same machine within a single queued job
submission.  The jobs are independent in the sense that each has its
own standard input and standard output, but each executes the same
binary.  The @strong{qmp-jobs} or @strong{jobs} option controls the
partitioning of the allocated processors.  This option takes one or
four integers, depending on whether you are using a switch view or a
mesh view of the machine, respectively.  With a single integer
@example
  -qmp-jobs n
@end example
the allocated processors are divided into @strong{n} equal sets of
processors.  Thus @strong{n} must be a divisor of the number of
allocated processors.  With four parameters
@example
  -qmp-jobs jx jy jz jt
@end example
the mesh with dimensions mx, my, mz, mt is subdivided.  Each integer
ji must be a divisor of mi.  The number of independent jobs is the
product n = jx * jy * jz * jt.

Under multijob operation it is necessary to distinguish the sets of
files for separate jobs.  Therefore, arguments @strong{stdin_path},
@strong{stdout_path}, and @strong{stderr_path} are required.  They are
taken to be the stems of the actual file names.  Each job is assigned
a two-digit sequence number nn as in 00, 01, 02, ... 99.  (It is
assumed that there will not be more than 100 such jobs.) The full file
names have the format
@example
   stdin_path.jnn  stdout_path.jnn  stderr_path.jnn
@end example
where @strong{nn} is the sequence number of the job.  In this way each
job has its unique set of files.

@node General description, Programming with MILC Code, Building the MILC Code, Top
@chapter General description
@cindex General description of the MILC Code

@cindex MILC Collaboration
@strong{The MILC Code} is a set of codes written in C developed by the MIMD
Lattice Computation (MILC) collaboration for doing simulations of four
dimensional SU(3) lattice gauge theory on MIMD parallel machines.
The MILC Code is publicly available for research purposes. Publications of work
done using this code or derivatives of this code should acknowledge this
use. @ref{Usage conditions}.

@menu
* Directory Layout::
* Overview of applications::
* Precision::
* Optional Compilation Macros::
@end menu

@node Directory Layout, Overview of applications, General description, General description
@section Directory Layout
@cindex Directory Layout

In the top-level directory there are six categories of subdirectories:
``applications,'' ``generic,'' ``include,'' ``libraries,'' ``doc,''
and ``file_utilities.''

@cindex libraries
Each @strong{application}, or major research project, has
its own directory. Examples of applications are @strong{ks_imp_dyn}
(dynamical simulations with a variety of staggered fermion actions)
and @strong{clover_invert2} (clover Dirac operator inversion and
spectroscopy). All applications share the @strong{libraries} directory
containing low-level linear algebra routines, the @strong{include}
directory containing header files shared by all applications, the
@strong{generic} directory containing high level routines that is more
or less independent of the physics, and a set of slightly more
specific @strong{generic_XXX} directories. Examples of
@strong{generic} code are the random number routines, the layout
routines for distributing lattice sites across the machine, and
routines to evaluate the plaquette or Polyakov loop.  The @strong{doc}
directory contains documentation and the @strong{file_utilities}
directory contains some code for manipulating files, including a code
(check_gauge) for doing some consistency checking of a gauge
configuration file and a code (v5_to_scidac) for converting MILC
formatted lattices to SciDAC or ILDG format.

Each application usually has several variants that appear as separate
targets in the make file.  For example, the @strong{ks_imp_dyn}
application can be compiled for hybrid Monte Carlo updating (su3_hmc)
or the R algorithm (su3_rmd).  All application directories have
separate sample input and output files for each target, labelled with
the name of the target: for example "test/su3_hmc.1.sample-in", etc., and a
corresponding output file "su3_hmc.1.sample-out", etc.)  The numbers
@strong{1} and @strong{2} refer to single and double precision.

You may unpack supporting code only for a specific application or you
may unpack the entire code.  The entire code consists of at least the
following directories.  
(@pxref{Building the MILC Code})

@sp 1
@center @emph{SUPPORT ROUTINES}
@table @strong
@item libraries:
Single processor linear algebra routines.  After building the code the
following libraries appear:
@itemize @bullet

@item complex.1.a and complex.2.a

Library of routines for complex numbers.

@item su3.1.a and su3.2.a

Library of routines for SU(3) matrix and vector operations.

@end itemize
@item include:
We list only some of the major headers here.

@itemize @bullet

@item config.h

Specification of processor configuration and operating system environment.

@item complex.h

Header file of definitions and macros for complex numbers.

@item comdefs.h

Header files for communications

@item dirs.h

Defines some standard macros for lattice directions.

@item gammatypes.h

Gamma matrix definitions.

@item generic_XXX.h

Header files and declarations for routines in directory generic_XXX

@item io_ksprop.h

Prototypes and definitions for staggered fermion I/O routines.

@item io_lat.h

Header file for routines for reading and writing lattices (the routines
are in @kbd{generic})

@item io_scidac.h

Prototypes and definitions for generic SciDAC I/O routines.

@item io_wprop.h

Prototypes and definitions for Dirac fermion I/O routines.

@item macros.h

Definitions required in all applications: field offsets, field pointers,
looping over variables...

@item prefetch.h prefetch_asm.h

Header files defining subroutine or macro calls for cache preloading.

@item random.h

Definition of structure for random number state.

@item su3.h

Header file of definitions and macros for SU(3) matrix and fermion
operations.

@end itemize
@item generic:
    Procedures common to most applications, such as communications,
    I/O, data layout, and field remapping.

@item generic_XXX:
    Procedures common to a subset of applications, including
    inverters and link fattening.
    Presently ``XXX'' includes 
   generic_clover, generic_form, generic_ks, generic_pg, generic_schroed, 
   and generic_wilson.

@end table

@node Overview of applications, Precision, Directory Layout, General description
@section Overview of Applications
@cindex Overview of Applications in Release Version
@table @strong
@item arb_overlap:
      Computes eigenvalues and eigenvectors of the overlap operator.

@item clover_dynamical:
	Simulations with dynamical clover fermions.  Variants
	include the "R", "phi" and hybrid Monte Carlo updating
	algorithms. 

@item clover_invert2:
      Inversion of the clover fermion matrix (conjugate gradient,
      MR, and BiCG algorithms) and measurements with clover or staggered
      quarks.  A wide variety of sources are supported. Lattices are
      supposed to be generated by someone else.

@item ext_src:
       Generates an extended source from a staggered or clover
       propagator.

@item file_utilities
       A variety of utilities for converting lattice file formats, running
       a checksum of a gauge file, and  comparing some binary files.

@item gauge_utilities
       A variety of utilities for manipulating the gauge field, including
       coordinate translations, gauge fixing, and boundary twists.

@item gluon_prop:
       Gluon and quark propagators.

@item hvy_qpot:
       Measures static quark potential as a function of separation.
       Also a variety of Wilson loops.

@item ks_eigen
       Compute eigenvalues of the staggered Dirac matrix using the
       Kalkreuter Rayleigh-Ritz method.

@item ks_imp_dyn:
@cindex Asqtad action
	Simulations with dynamical staggered (Kogut-Susskind) 
        fermions.  There are a variety of possible actions, including
        the original unimproved Kogut-Susskind action and the
        asqtad action.  One may choose a single degenerate mass or two
        different masses. Make targets
	include the "R", "phi" and hybrid Monte Carlo updating
	algorithms.  Measurements of a few quantities are included: 
        plaquette, Polyakov loop,
@tex 
$<\psi\bar\psi>$ 
@end tex

@item ks_imp_rhmc:
@cindex rational function
@cindex RHMC
@cindex HISQ
        Dynamical RHMC code for staggered fermions.  In addition to
        a variety of staggered actions, the highly-improved-staggered-quark
        (HISQ) action is also supported.  A subdirectory @strong{remez-milc}
        contains a utility for generating parameters of the rational functions
        needed by the RHMC codes.

@item ks_imp_utilities:
      Test code for the staggered fermion force and staggered inverter.

@item ks_measure:
      Calculate @tex 
$<\psi\bar\psi>$ 
@end tex
     and a wide variety of quantities used in staggered
     thermodynamics, for the equation of state and quark number
      susceptibilities at zero and nonzero chemical potential

@item ks_spectrum
      Inversion of the staggered fermion matrix for asqtad and HISQ actions
      and measurements of correlators. A wide variety of sources are supported.
      Lattices are supposed to be generated by someone else.

@item pure_gauge:
      Simulation of the unimproved pure gauge theory, using
      microcanonical overrelaxed and quasi-heat bath, or hybrid Monte Carlo
      algorithms. Not much is measured.

@item schroed_cl_inv:
      Schroedinger functional code for clover quarks.

@item smooth_inst:
      Compute topological charge with smearing.

@item symanzik_sl32
      Pure gauge Monte Carlo for an arbitrary hypercubic action, including
      the improved gauge actions.

@end table

The top level directory contains a @file{README} file with specific information
on how to @kbd{make} an application code (@pxref{Building the MILC Code}).

@node Precision, , Overview of applications, General description
@section Precision
@cindex Precision
@cindex Compilation Macros
@cindex Mixed Precision

The MILC code can be compiled in single precision or double precision,
depending on whether the make macro @strong{PRECISION} is set to
@strong{1} or @strong{2}.  You do this by editing the
@strong{Makefile}.  The effect of this setting is global for the
entire compilation.  All of the MILC floating types are set to either
single or double precision types.

An exception to the global precision rule is that gauge configuration
and propagator files are always written in single precision.

For some applications compiled with SciDAC QOPQDP, the code now
supports mixed precision computation.  That is, you may set a global
precision with the @strong{PRECISION} macro, but some applications
accept a run-time selection that allows some supporting calculations
to be done with a different precision.  This has proven to be
efficient for molecular dynamics evolution in which the fermion force
calculation can be done in single precision, but the gauge field
updating is then done in double precision.


@node Optional Compilation Macros, ,Programming with MILC Code, General description
@section Optional Compilation Macros
@cindex Macros, Optional
@cindex Compilation Macros

We list some macros affecting compilation of various sections of the
code that may be defined by user option.  Most of these macros are
described with comments in the generic @strong{Makefile}.  

Some application-specific macros are defined in the application file
@strong{Make_template}. Finally, in the application
@strong{Make_template} files some choices are exercised by selecting
one of a few interchangeable object files.  Look for comments in those
files.

Note that there are both @strong{make} macros and compiler macros in
this list.  The compiler macros in this list are distinguished by the
compiler flag @strong{-D}.

@table @strong

@item SciDAC package options
@itemize @bullet

@item WANTQOP
@cindex QOP
   Use optimized QOPQDP routines.  The choices are @strong{true} to
   get this option and blank (nothing) to use the native MILC versions
   instead.

@item WANTQIO
@cindex QIO
    The choices are @strong{true} and blank (nothing).  Enables
reading and writing of SciDAC (QIO/LIME) formatted files.  This option
is necessary for writing staggered and clover propagators.

@item WANTQMP
@cindex QMP
    The choices are @strong{true} and blank (nothing).  Causes
communication to take place through QMP calls.  Then, depending on
which QMP library you link with the code, you may get an
architecture-specific implementation or a generic MPI implementation.

@end itemize

@item GPU package options

The code includes development-grade support for GPU computations,
currently only for single-GPU operation, but multi-GPU operation is
planned.  The QUDA package is required.  All the basic modules for
staggered fermion molecular dynamics evolution and spectroscopy are
provided.

@itemize @bullet

@item USE_CG_GPU
@cindex GPU staggered conjugate gradient

@item USE_FL_GPU
@cindex GPU fat link construction

@item USE_REUNIT_GPU
@cindex GPU reunitarization
  This step is used for constructing HISQ links.

@item USE_FF_GPU
@cindex GPU staggered fermion force

@item USE_GF_GPU
@cindex GPU gauge force

@end itemize

@item Inlining
@cindex Inlining

    Some of the MILC library routines can be invoked through inline
C-coded macros.  This method avoids the overhead cost of a procedure
call, but increases the size of the compiled code somewhat.  It is
usually a good idea.  Some processors have an SSE instruction set.  In
the interest of efficient computation on those architectures, some
library routines are available as inline SSE assembly code.  Inline
assembly code can be interpreted by some compilers (gcc).  Before
trying them, you might see whether the chosen compiler doesn't already
have an option that accesses this instruction set.  Both sets of
inline macros can be invoked globally in the computation or
selectively.  ``Globally'' means that every procedure call throughout
the code is inlined.  ``Selectively'' means only some of the procedure
calls are inlined.  To obtain the latter result, you have to edit the
code by hand, in each procedure call, changing the procedure name to
its corresponding macro name.

The macros that invoke these inline codes are as follows

@itemize @bullet

@item -DC_GLOBAL_INLINE
     Invokes C-inline versions of library routines as available.

@item -DSSE_GLOBAL_INLINE
     Invokes SSE assembly-coded inline versions of library routines as
available.  Where both C and SSE assembly versions are available, the
SSE version takes precedence.

@item -DC_INLINE
     Invoke C-inline versions selectively.  You then must edit the code to 
get just the routines you want.

@item -DSSE_INLINE
@cindex SSE
     Invoke SSE assembly-coded inline versions selectively.  You then must 
edit the code to get just the routines you want.

@end itemize

@item Timing, profiling, and debugging
@cindex Timing
@cindex Profiling
@cindex Debugging

@itemize @bullet

@item -DCGTIME
   Print timing information for the conjugate gradient solvers.

@item -DFFTIME
   Print timing information for the staggered fermion force routines.

@item -DFLTIME
   Print timing information for the staggered link fattening routines.

@item -DGFTIME
   Print timing information for the gauge force routines.

@item -DIOTIME
   Print timing information for I/O routines.

@item -DPRTIME
   Print timing information for various computational steps. (Some applications.)

@item -DREMAP
   In some applications it is necessary to remap data in lattice fields
   because of different layout conventions.  This macro gives code that prints
   timing information for such remapping.

@item -DQDP_PROFILE
   Print profiling information for QDP/C routines.

@item -DCOM_CRC
   Do checksums on all gather operations.  Slows performance somewhat.

@item -DCHECK_MALLOC
   Mainly for developers for analyzing heap utilization.  In
conjunction with the script @strong{check_malloc.pl}, checks the
consistency of all malloc/free activity.  (Produces voluminous
output.)

@item -DCG_DEBUG
   Show detailed progress of the conjugate gradient solver.

@item -DCG_OK
   Print summary information about the conjugate gradient solver.

@end itemize

@item Grid layout
@cindex Layout
@cindex qmp-geom

   The layout subroutines normally decide which processor memory gets
which lattice sites.  The @file{layout_hyper_prime} routine does this
by dividing the lattice into hypercubes.  This is a natural concept
for grid-based communication networks, but it is also used in
switch-based networks.  A few applications in the code now allow you
to control the geometry by hand with the @strong{FIX_NODE_GEOM} macro.
If you compile with the SciDAC QMP pacakage in any application, the
same control is achieved through the @strong{-qmp-geom} command-line
option.

The SciDAC QIO utilities support parallel I/O through I/O partitions.
That is, for the purpose of I/O the processors are divided into
disjoint subsets called I/O partitions.  In each partition there is
one processor designated for I/O.  It works with its own exclusive
file and distributes/collects data from sites within its I/O
partition.  The QIO suite also contains scalar processor code for
converting files from partitioned format to single-file format and
back.  A few applications in the code now support this feature,
enabled through the @strong{FIX_IONODE_GEOM} macro.  The I/O
partitions are created as hypercubes in the space of nodes
(processors).  Since the layout of the I/O partitions must be
commensurate with the hypercubic organization of the processors, we
require fixing the node geometry with @strong{FIX_NODE_GEOM} when
using I/O partitions.

@itemize @bullet

@item -DFIX_NODE_GEOM

   For some applications only.  Provide for specifying the x, y, z,
and t dimensions of the processors, viewing the allocated machine as a
4D grid of processors.
   
@item -DFIX_IONODE_GEOM
@cindex I/O partitions

   For some applications only.  Provide for specifying the x, y, z,
and t dimensions of the I/O partitions.

@end itemize

@item Staggered CG inverter and Dslash optimizations
@cindex Optimization

@itemize @bullet

@item -DDBLSTORE_FN

    Double store backward links to optimize Dslash.  Uses more memory.

@item -DD_FN_GATHER13

    For staggered Fat-Naik actions.  Assume that the next neighbor is
gathered when the third neighbor is gathered.  This is always the case
in hypercubic layouts when the local sublattice dimensions are three
or more.  If the layout is incompatible with this option, the code
halts.

@item -DFEWSUMS

   Pair up global sums in the conjugate gradient algorithm to save global
   reduction time.

@end itemize

@item Multimass CG solvers
@cindex Conjugate gradient - multimass

   There are a variety of options for doing multimass CG solves, some
of them experimental and debugging.  This list will be pruned in future
versions.

@itemize @bullet

@item -DKS_MULTICG=HYBRID

    Solve with multimass and polish each solution with single-mass CG.
    Good for reliability.

@item -DKS_MULTICG=FAKE

    Solve by iterating through the single-mass solver.  For debugging.

@end itemize


@item Multifermion force routines
@cindex Fermion force (multiple)

   There are a variety of options for computing the multimass fermion
force needed for the RHMC algorithm.

@itemize @bullet

@item -DKS_MULTIFF=FNMAT

    First sum the outer products of the sources and work with matrix
parallel transporters.  This method is generally more efficient.

@item -DKS_MULTIFF=FNMATREV

    Traverses the path in reverse order.

@item -DKS_MULTIFF=ASVEC

    Do parallel transport of the list of source vectors, rather than
parallel transporting a matrix.  The @strong{-DVECLENGTH=n} macro sets
the maximum number of source vectors processed at a time.  The fermion
force routine is then called as many times as needed to process them
all.  For HISQ actions, the only option is @strong{FNMAT}.

@end itemize

@item RHMC molecular dynamics algorithms
@cindex RHMC

   These macros control only the RHMC molecular dynamics algorithms.
They determine the integration algorithm and the relative frequency of
gauge and fermion field updates.

@itemize @bullet

@item -DINT_ALG=INT_LEAPFROG
@item -DINT_ALG=INT_LEAPFROG
@item -DINT_ALG=INT_OMELYAN
@item -DINT_ALG=INT_2EPS_3TO1
@item -DINT_ALG=INT_2EPS_2TO1
@item -DINT_ALG=INT_2G1F
@item -DINT_ALG=INT_3G1F
@item -DINT_ALG=INT_4MN4FP
@item -DINT_ALG=INT_4MN5FV
@item -DINT_ALG=INT_FOURSTEP
@item -DINT_ALG=INT_PLAY

@end itemize


@item Dirac (clover) inverter choices
@cindex Dirac inverter
@cindex Clover inverter

  There are a variety of Dirac (clover) solvers

@itemize @bullet

@item -DCL_CG=BICG  Biconjugate gradient
@item -DCL_CG=CG    Standard CG
@item -DCL_CG=MR    Minimum residue
@item -DCL_CG=HOP   Hopping

@end itemize

@end table

@node Programming with MILC Code, Writing Your Own Application, General description, Top
@chapter Programming with MILC Code
@cindex Programming with MILC Code


These notes document some of the features of the MILC QCD code.  They
are intended to help people understand the basic philosophy and
structure of the code, and to outline how one would modify existing
applications for a new project.

@menu
* Header files::
* Global variables::
* Lattice storage::
* Data types::
* Library routines::
* Moving around in the lattice::
* Accessing fields at other sites::
* Details of gathers and creating new ones::
* Distributing sites among nodes::
* Reading and Writing Lattices and Propagators::
* Random numbers::
* A Sample Applications Directory::
* Bugs and features::
@end menu

@node Header files, Global variables, Programming with MILC Code, Programming with MILC Code
@section  Header files
@cindex header files

Various header files define structures, macros, and global
variables.  The minimal set at the moment is:
@cindex config.h
@cindex comdefs.h
@cindex complex.h
@cindex defines.h
@cindex dirs.h
@cindex io_lat.h
@cindex lattice.h
@cindex macros.h
@cindex params.h
@cindex su3.h

@table @file
@item config.h
specifies processor and operating-system-dependent macros

@item comdefs.h 
macros and defines for communication routines (@pxref{Library routines}).

@item complex.h	
declarations for complex arithmetic (@pxref{Library routines}).

@item defines.h
defines macros specific to an application.  Macros that are
independent of the site structure can also be defined here.  Compiler
macros common to all targets are also defined here.

@item dirs.h
defines macros specifying lattice directions

@item lattice.h
defines lattice fields and global variables specific to an
application, found in applications directories

@item macros.h
miscellaneous macros including loop control

@item params.h
defines the parameter structure for holding parameter values read from
standard input, such as the lattice size, number of iterations, etc.

@item su3.h 
declarations for SU(3) operations, eg. matrix multiply (@pxref{Library
routines}).

@end table

The @strong{lattice.h} file is special to an application directory.
It defines the site structure (fields) for the lattice. The files
@strong{defines.h} and @strong{params.h} are also peculiar to an
application directory.  The other header files are stored in the
@strong{include} directory and not changed.

@vindex Include files. 

The local header @strong{defines.h} must be included near the top of the @strong{lattice.h} file.  Other headers, of course, should be
included to declare data types appearing in the @strong{lattice.h}
file.

@vindex EXTERN
@vindex CONTROL
In C global variables must be defined as ``extern'' in all but one of
the compilation units.  To make this happen with global variables in
@strong{lattice.h}, we use the macro prefix @kbd{EXTERN}.  The macro
is normally defined as "extern", but when the macro @kbd{CONTROL} is
defined, @kbd{EXTERN} becomes null. The effect is to reserve storage
in whichever file @kbd{CONTROL} is defined.  We do this typically [qin
the file containing the @emph{main()} program, which is typically part
of a file with a name like @kbd{control.c}. (C++ would fix this
nonsense).

@node Global variables, Lattice storage, Header files, Programming with MILC Code
@section  Global variables
@cindex Global variables

@vindex beta
@vindex epsilon
@vindex even_sites_on_node
@vindex fixflag
@vindex iseed
@vindex mass
@vindex nflavors
@vindex nflavors1, nflavors2
@vindex number_of_nodes
@vindex nx,ny,nz,nt
@vindex odd_sites_on_node
@vindex saveflag
@vindex *savefile 
@vindex saveflag 
@vindex *startfile
@vindex sites_on_node
@vindex startflag
@vindex steps
@vindex this_node
@vindex total_iters 
@vindex trajecs
@vindex volume
@vindex warms
A number of global variables are typically available.  Most of them are
declared in @file{lattice.h}. Unless specified, these variables are
initialized in the function @kbd{initial_set()}.  Most of them take
values from a parameter input file at run time.

@table @kbd
@item int nx,ny,nz,nt
lattice dimensions
@item int volume
volume = nx * ny * nz * nt
@item int iseed
random number seed
@end table

Other variables are used to keep track of the relation between sites of the
lattice and nodes:
@table @kbd
@item int this_node
number of this node
@item int number_of_nodes
number of nodes in use
@item int sites_on_node	
number of sites on this node. [@emph{This variable is set in:}
@kbd{setup_layout()}]
@item int even_sites_on_node
number of evensites on this node. [@emph{This variable is set in:}
@kbd{setup_layout()}]
@item int odd_sites_on_node
number of odd sites on this node. [@emph{This variable is set in:}
@kbd{setup_layout()}]
@end table


@cindex @file{setup.c}
@vindex params
Other variables are not fundamental to the layout of the
lattice but vary from application to application. These dynamical
variables are part of a @kbd{params} struct, defined in the required
header @cindex params.h, which is passed between nodes by
@kbd{initial_set()} in @file{setup.c} For example, a pure gauge
simulation might have a @kbd{params} struct like this:
@example
/* structure for passing simulation parameters to each node */
typedef struct @{
        int nx,ny,nz,nt;  /* lattice dimensions */
        int iseed;      /* for random numbers */
        int warms;      /* the number of warmup trajectories */
        int trajecs;    /* the number of real trajectories */
        int steps;      /* number of steps for updating */
        int stepsQ;     /* number of steps for quasi-heatbath */
        int propinterval;  /* number of trajectories between measurements */
        int startflag;  /* what to do for beginning lattice */
        int fixflag;    /* whether to gauge fix */
        int saveflag;   /* what to do with lattice at end */
        float beta;     /* gauge coupling */
        float epsilon;  /* time step */
        char startfile[80],savefile[80];
@} params; 
@end example
These run-time variables are usually loaded by a loop over
@kbd{readin()} defined in @file{setup.c}.


@node Lattice storage, Data types, Global variables, Programming with MILC Code
@section Lattice storage 
@cindex Lattice storage
@cindex site
@vindex site
@cindex lattice[]

The original MILC code put all of the application variables that live
on a lattice site in a huge structure called the ``site'' structure.
The @kbd{lattice} variable was then an array of @kbd{site} objects.
We call this data structure``site major'' order.  Experience has shown
that it is usually better for cache coherence to store each lattice
field in its own separate linear array.  We call this ``field major''
order.  Thus we are in the process of dismantling the site structure
in each application.  Eventually all fields will be stored in
field-major order.  In the transitional phase most utilities in the
code support both data structures.

The @kbd{site} structure is defined in @file{lattice.h} (@pxref{Header
files}). Each @emph{node} of the parallel machine has an array of such
structures called @kbd{lattice}, with as many elements as there are
sites on the @emph{node}.  In scalar mode there is only one
@emph{node}.  The @kbd{site} structure looks like this:
@example
typedef struct @{
    /* The first part is standard to all programs */
        /* coordinates of this site */
        short x,y,z,t;
        /* is it even or odd? */
        char parity;
        /* my index in the lattice array */
        int index;

    /* Now come the physical fields, application dependent. We will 
       add or delete whatever we need. This is just an example. */
        /* gauge field */
        su3_matrix link[4];

        /* antihermitian momentum matrices in each direction */
         anti_hermitmat mom[4];

         su3_vector phi; /* Gaussian random source vector */ 
@} site; 
@end example

At run time space for the lattice sites is allocated dynamically,
typically as an array @kbd{lattice[i]}, each element of which is a site
structure.  Thus, to refer to the @kbd{phi} field on a particular
lattice site, site "@kbd{i}" on this node, you say
@example
   lattice[i].phi,
@end example
and for the real part of color 0
@example
   lattice[i].phi.c[0].real,
@end example
etc. You usually won't need to know the relation between the index
@kbd{i} and a the location of a particular site (but see
 (@pxref{Distributing sites among nodes} for how to figure out
the index @kbd{i}). 

In general, looping over sites is done using a pointer to the
 site, and then you would
refer to the field as:
@example
site *s; ...  
/* s gets set to &(lattice[i]) */ 
s->phi
@end example

The coordinate, parity and index fields are used by the gather routines
and other utility routines, so it is probably a bad idea to alter
them unless you want to change a lot of things.  Other data can be
added or deleted with abandon.

The routine @kbd{generic/make_lattice()} is called from @kbd{setup()} to
allocate the lattice on each node.

@cindex neighbor[]
In addition to the fields in the @kbd{site} structure, there are two
sets of vectors whose elements correspond to lattice sites. 
They are hidden in the communications routines and you are likely never to
encounter them. These are
the eight vectors of integers:
@example
int *neighbor[MAX_GATHERS]
@end example
@kbd{neighbor[XDOWN][i]} is the index of the site in the @strong{XDOWN}
direction from the i'th site on the
node, if that site is on the same node. If the neighboring site is on
another node, this pointer will be @strong{NOT_HERE (= -1)}. These
vectors are mostly used by the gather routines.

@cindex gen_pt[]
There are a number of important vectors of pointers used for accessing
fields at other (usually neighboring) neighboring sites,
@example
char **gen_pt[MAX_GATHERS]
@end example
These vectors of pointers are declared in @file{lattice.h} and allocated
in @kbd{generic/make_lattice()}. They are filled by the gather routines,
@kbd{start_gather()} and @kbd{start_general_gather()}, with pointers to
the gathered field. @xref{Accessing fields at other sites}.  You use one
of these pointer vectors for each simultaneous gather you have going.


This storage scheme seems to allow the easiest coding, and likely the
fastest performance.  It certainly makes gathers about as easy as
possible.  However, it is somewhat wasteful of memory, since all
fields are statically allocated.  Also, there is no mechanism for
defining a field on only even or odd sites.

@node Data types, Library routines, Lattice storage, Programming with MILC Code
@section Data types
@cindex Data types

Various data structures have been defined for QCD computations, which
we now list.   You may define new ones if you wish.
In names of members of structure, we  use the following conventions:
@table @kbd
@item c 
means color, and has an index which takes three values (0,1,2).
@item d
means Dirac spin, and its index takes four values (0-3).
@item e
means element of a matrix, and has two indices which take three values -
row and column--(0-2),(0-2)
@end table

Complex numbers: (in @file{complex.h}).  Since present-day C compilers have
native complex types these MILC types will be replaced in future code
with native types.
@example
typedef struct @{ /* standard complex number declaration for single- */
   float real;    /* precision complex numbers */
   float imag; 
@} complex;

typedef struct @{ /* standard complex number declaration for double- */
   double real;   /* precision complex numbers */
   double imag; 
@} double_complex;
@end example

Three component complex vectors, 3x3 complex matrices, and 3x3
antihermitian matrices stored in triangular (compressed) format.  (in
@file{su3.h})
@example
typedef struct @{ complex e[3][3]; @} su3_matrix; 
typedef struct @{ complex c[3]; @} su3_vector; 
typedef struct @{
  float m00im,m11im,m22im;
  complex m01,m02,m12; 
@} anti_hermitmat;
@end example

Wilson vectors have both Dirac and color indices:
@example
  typedef struct @{ su3_vector d[4]; @} wilson_vector;
@end example
Projections of Wilson vectors
@tex
$(1 \pm \gamma_\mu)\psi$
@end tex
@example
  typedef struct @{ su3_vector h[2]; @} half_wilson_vector;
@end example
A definition to be used in the next definition:
@example
  typedef struct @{ wilson_vector d[4]; @} spin_wilson_vector;
@end example
A four index object --- source spin and color by sink spin and color:
@example
  typedef struct @{ spin_wilson_vector c[3]; @} wilson_propagator
@end example

Examples:
@example
su3_vector phi; /* declares a vector */ 
su3_matrix m1,m2,m3;         /* declares 3x3 complex matrices */ 
wilson_vector wvec;          /* declares a Wilson quark vector */

phi.c[0].real = 1.0;         /* sets real part of color 0 to 1.0 */ 
phi.c[1] = cmplx(0.0,0.0);   /* sets color 1 to zero (requires
                                including "complex.h" */ 
m1.e[0][0] = cmplx(0,0);     /* refers to 0,0 element */ 
mult_su3_nn( &m1, &m2, &m3); /* subroutine arguments are usually
                                addresses of structures */
wvec.d[2].c[0].imag = 1.0;   /* How to refer to imaginary part of
                                spin two, color zero. */
@end example

@node Library routines, Moving around in the lattice, Data types, Programming with MILC Code
@section Library routines
@cindex Library routines

@subsection Complex numbers
@file{complex.h} and @file{complex.a} contain macros and subroutines for
complex numbers.  For example:
@example
complex a,b,c; 
CMUL(a,b,c); /* macro: c <- a*b */
@end example
Note that all the subroutines (@kbd{cmul()}, etc.) take addresses as
arguments, but the macros generally take the structures themselves.
These functions have separate versions for single and double precision
complex numbers.  The macros work with either single or double precison
(or mixed).  @file{complex.a} contains:
@example
complex cmplx(float r, float i);      /* (r,i) */ 
complex cadd(complex *a, complex *b); /* a + b */ 
complex cmul(complex *a, complex *b); /* a * b */ 
complex csub(complex *a, complex *b); /* a - b */ 
complex cdiv(complex *a, complex *b); /* a / b */ 
complex conjg(complex *a);            /* conjugate of a */ 
complex cexp(complex *a);             /* exp(a) */ 
complex clog(complex *a);             /* ln(a) */ 
complex csqrt(complex *a);            /* sqrt(a) */
complex ce_itheta(float theta);       /* exp( i*theta) */

double_complex dcmplx(double r, double i);                  /* (r,i) */ 
double_complex dcadd(double_complex *a, double_complex *b); /* a + b */ 
double_complex dcmul(double_complex *a, double_complex *b); /* a * b */ 
double_complex dcsub(double_complex *a, double_complex *b); /* a - b */ 
double_complex dcdiv(double_complex *a, double_complex *b); /* a / b */ 
double_complex dconjg(double_complex *a);          /* conjugate of a */ 
double_complex dcexp(double_complex *a);                   /* exp(a) */ 
double_complex dclog(double_complex *a);                    /* ln(a) */ 
double_complex dcsqrt(double_complex *a);                 /* sqrt(a) */ 
double_complex dce_itheta(double theta);            /* exp( i*theta) */
@end example
and macros:
@example
CONJG(a,b)        b = conjg(a)
CADD(a,b,c)       c = a + b
CSUM(a,b)         a += b
CSUB(a,b,c)       c = a - b
CMUL(a,b,c)       c = a * b
CDIV(a,b,c)       c = a / b
CMUL_J(a,b,c)     c = a * conjg(b)
CMULJ_(a,b,c)     c = conjg(a) * b
CMULJJ(a,b,c)     c = conjg(a*b)
CNEGATE(a,b)      b = -a
CMUL_I(a,b)       b = ia
CMUL_MINUS_I(a,b) b = -ia
CMULREAL(a,b,c)   c = ba with b real and a complex
CDIVREAL(a,b,c)   c = a/b with a complex and b real
@end example

@subsection SU(3) operations
@file{su3.h} and @file{su3.a} contain a plethora of
functions for SU(3) operations.
For example:
@example
void mult_su3_nn(su3_matrix *a, su3_matrix *b, su3_matrix *c); 
/* matrix multiply, no adjoints
   *c <- *a * *b (arguments are pointers) */

void mult_su3_mat_vec_sum(su3_matrix *a, su3_vector *b, su3_vector *c);
/* su3_matrix times su3_vector multiply and add to another su3_vector
   *c <- *A * *b + *c */
@end example
There have come to be a great many of these routines, too many to keep a
duplicate list of here.  Consult the include file @file{su3.h} for a
list of prototypes and description of functions. 

@node Moving around in the lattice, Accessing fields at other sites, Library routines, Programming with MILC Code
@section Moving around in the lattice
@cindex Moving around in the lattice

@vindex field_offset
@vindex field_pointer
@vindex F_OFFSET
@vindex F_PT
Various definitions, macros and routines exist for dealing with the
lattice fields.  
The definitions and macros (defined in @file{dirs.h}) are:
@example
/* Directions, and a macro to give the opposite direction */ 
/* These must go from 0 to 7 because they will be used to index an
   array. */ 
/* Also define NDIRS = number of directions */
#define XUP 0 
#define YUP 1 
#define ZUP 2 
#define TUP 3 
#define TDOWN 4 
#define ZDOWN 5 
#define YDOWN 6 
#define XDOWN 7 
#define OPP_DIR(dir) (7-(dir))  /* Opposite direction */ 
                                /* for example, OPP_DIR(XUP) is XDOWN */
/* number of directions */ 
#define NDIRS 8 
@end example

The parity of a site is @strong{EVEN} or @strong{ODD}, where
@strong{EVEN} means (x+y+z+t)%2=0.  Lots of routines take @strong{EVEN},
@strong{ODD} or @strong{EVENANDODD} as an argument. Specifically (in hex):
@example
#define EVEN 0x02 
#define ODD 0x01 
#define EVENANDODD 0x03
@end example

Often we want to use the name of a field as an argument to a routine, as
in @kbd{dslash(chi,phi)}. Often these fields are members of the structure 
@kbd{site}, and such variables can't be used directly as arguments
 in C. Instead, we use a
 macro to convert the name of a field
into an integer, and another one to convert this integer back into an
address at a given site.  A type @kbd{field_offset}, which is secretly
an integer, is defined to help make the programs clearer.

@strong{F_OFFSET}(@kbd{fieldname}) gives the offset in the site structure of
the named field. @strong{F_PT}(@kbd{*site}, @kbd{field_offset}) gives the
address of the field whose offset is @kbd{field_offset} at the site
@kbd{*site}. An example is certainly in order:
@example
int main() @{
  copy_site( F_OFFSET(phi), F_OFFSET(chi) );
  /* "phi" and "chi" are names of su3_vector's in site. */ 
@}

/* Copy an su3_vector field in the site structure over the whole lattice */
int copy_site(field_offset off1, field_offset off2) @{
  int i;
  site *s;
  su3_vector *v1,*v2;

  for(i=0;i<nsites_on_node;i++) @{ /* loop over sites on node */
     s = &(lattice[i]); /* pointer to current site */
     v1 = (su3_vector *)F_PT( s, off1); /* address of first vector */
     v2 = (su3_vector *)F_PT( s, off2);
     *v2 = *v1; /* copy the vector at this site */
  @} 
@}
@end example


For ANSI prototyping, you must typecast the result of the
@strong{F_PT} macro to the appropriate pointer type.  (It naturally
produces a character pointer).  The code for copy_site could be much
shorter at the expense of clarity.  Here we use a macro to be defined
below.
@example
/* Copy an su3_vector field in the site structure over the whole lattice */
void copy_site(field_offset off1, field_offset off2) @{
  int i;
  site *s;
  FORALLSITES(i,s) @{
    *(su3_vector *)F_PT(s,off1) = *(su3_vector *)F_PT(s,off2);
  @} 
@}
@end example

Since we now recommend field-major order for the fields, here is
the same example, but for the recommended practice:
@example
/* Copy an su3_vector field in field-major order over the whole lattice */
void copy_field(su3_vector vec1[], su3_vector vec2[]) @{
  int i;
  site *s;
  FORALLSITES(i,s) @{
    vec1[i] = vec2[i];
  @} 
@}
@end example

The following macros are not necessary, but are very useful.  You may
use them or ignore them as you see fit. Loops over sites are so common
that we have defined macros for them ( in
@file{include/macros.h}). These macros use an integer and a site pointer,
which are available inside the loop.  The site pointer is especially
useful for accessing fields at the site.
@example
/* macros to loop over sites of a given parity, or all sites on a node.
   Usage:
        int i;
        site *s;
        FOREVENSITES(i,s) @{
            Commands go here, where s is a pointer to the current
            site and i is the index of the site on the node.
            For example, the phi vector at this site is "s->phi".
        @} */ 
#define FOREVENSITES(i,s) \
    for(i=0,s=lattice;i<even_sites_on_node;i++,s++)
#define FORODDSITES(i,s) \
    for(i=even_sites_on_node,s= &(lattice[i]);i<sites_on_node;i++,s++)
#define FORALLSITES(i,s) \
    for(i=0,s=lattice;i<sites_on_node;i++,s++)
#define FORSOMEPARITY(i,s,parity) \
    for( i=((choice)==ODD ? even_sites_on_node : 0 ),  \
    s= &(lattice[i]); \
    i< ( (choice)==EVEN ? even_sites_on_node : sites_on_node); \
    i++,s++)
@end example
The first three of these macros loop over even, odd or all sites on the
node, setting a pointer to the site and the index in the array.  The
index and pointer are available for use by the commands inside the
braces.  The last macro takes an additional argument which should be one
of @strong{EVEN}, @strong{ODD} or @strong{EVENANDODD}, and loops over
sites of the selected parity.

@node Accessing fields at other sites, Details of gathers and creating new ones, Moving around in the lattice, Programming with MILC Code
@section Accessing fields at other sites
@cindex Accessing fields at other sites

In the examples thus far each node fetches and stores field values
only on its own sites.  To fetch field values at other sites, we use
gather routines.  These are portable in the sense that they will look
the same on all the machines on which this code runs, although what is
inside them is quite different.  All of these routines return pointers
to fields.  If the fields are on the same node, these are just
pointers into the lattice, and if the fields are on sites on another
node some message passing takes place.  Because the communcation
routines may have to allocate buffers for data, it is necessary to
free the buffers by calling the appropriate cleanup routine when you
are finished with the data.  These routines are in @file{com_XXXXX.c},
where @kbd{XXXXX} is either @kbd{vanilla} for a scalar machine,
@kbd{mpi} for MPI operation, @kbd{qmp} for QMP support.

The four standard routines for moving field values are
@kbd{start_gather_site} and @kbd{start_gather_field}, which start the
data exchange for either data in the site structure or in field-major
order, @kbd{wait_gather}, which interrupts the processor until the
data arrives, and @kbd{cleanup_gather}, which frees the memory used by
the gathered data.  A special @kbd{msg_tag} structure is used to
identify the gather.  If you are doing more than one gather at a time,
just use different @kbd{*msg_tags} for each one to keep them straight.

The abstraction used in gathers is that sites fetch data from sites,
rather than nodes from nodes.  In this way the physical distribution of
data among the nodes is hidden from the higher level call.  Any
one-to-one mapping of sites onto sites can be used to define a gather
operation, but the most common gather fetches data from a neigboring
site in one of the eight cardinal directions.  The result of a gather is
presented as a list of pointers.  Generally one of the @kbd{gen_pt[0]},
etc. arrays is used. (@pxref{Lattice storage}). On each site, or on each
site of the selected parity, this pointer either points to an on-node
address when the required data is on-node or points to an address in a
communications buffer when the required data has been passed from
another node.

These routines use asynchronous sends and receives when possible, so it
is possible to start one or more gathers going, and do something else
while awaiting the data.

Here are the four gather routines:

@example
/* "start_gather_site()" starts asynchronous sends and receives required
    to gather neighbors. */

msg_tag * start_gather_site(offset,size,direction,parity,dest);
  /* arguments */
  field_offset offset;  /* which field? Some member of structure "site" */
  int size;            /* size in bytes of the field
                            (eg. sizeof(su3_vector))*/
  int direction;       /* direction to gather from. eg XUP */
  int parity;          /* parity of sites whose neighbors we gather.
                            one of EVEN, ODD or EVENANDODD. */ 
  char * dest;         /* one of the vectors of pointers */


msg_tag * start_gather_field(void *field,size,direction,parity,dest);
  /* arguments */
  void *field;         /* which field? Some member of an array */
  int size;            /* size in bytes per site of the field
                            (eg. sizeof(su3_vector))*/
  int direction;       /* direction to gather from. eg XUP */
  int parity;          /* parity of sites whose neighbors we gather.
                            one of EVEN, ODD or EVENANDODD. */ 
  char * dest;         /* one of the vectors of pointers */


/* "wait_gather()" waits for receives to finish, insuring that the
   data has actually arrived.  The argument is the (msg_tag *) returned
   by start_gather. */

void wait_gather(msg_tag *mbuf);


/* "cleanup_gather()" frees all the buffers that were allocated, WHICH
    MEANS THAT THE GATHERED DATA MAY SOON DISAPPEAR. */

void cleanup_gather(msg_tag *mbuf);
@end example

Nearest neighbor gathers are done as follows. In the first example
we gather @kbd{phi} at all even sites from the neighbors in the
@strong{XUP} direction.  (@emph{Gathering at @strong{EVEN} sites means
that @kbd{phi} at odd sites will be made available for computations at
@strong{EVEN} sites.})
@example
msg_tag *tag; 
site *s; 
int i;
su3_vector *phi;

phi = (su3_vector *)malloc(sites_on_node * sizeof(su3_vector);

...

tag = start_gather_field( phi, sizeof(su3_vector), XUP,
                         EVEN, gen_pt[0] );

/* do other stuff */

wait_gather(tag); 
/* *(su3_vector *)gen_pt[0][i] now contains the address of the
    phi vector (or a copy therof) on the neighbor of site i in the
    XUP direction for all even sites i.  */

FOREVENSITES(i,s) @{ 
/* do whatever you want with it here.
   (su3_vector *)(gen_pt[0][i]) is a pointer to phi on
   the neighbor site. */ 
@}

cleanup_gather(tag); 
/* subsequent calls will overwrite the gathered fields. but if you 
don't clean up, you will eventually run out of space */
@end example


This second example gathers @kbd{phi} from two directions at once:
@example
msg_tag *tag0,*tag1; 
tag0 = start_gather_site( phi, sizeof(su3_vector), XUP,
                          EVENANDODD, gen_pt[0] ); 
tag1 = start_gather_site( phi, sizeof(su3_vector), YUP,
                          EVENANDODD, gen_pt[1] );

/** do other stuff **/

wait_gather(tag0); 
/* you may now use *(su3_vector *)gen_pt[0][i], the
   neighbors in the XUP direction. */

wait_gather(tag1); 
/* you may now use *(su3_vector *)gen_pt[1][i], the
   neighbors in the YUP direction. */

cleanup_gather(tag0); 
cleanup_gather(tag1);
@end example

Of course, you can also simultaneously gather different fields, or
gather one field to even sites and another field to odd sites.  Just be
sure to keep your @kbd{msg_tag} pointers straight.  The internal
workings of these routines are far too horrible to discuss here.
Consult the source code and comments in @file{com_XXXXX.c} if you must.

Another predefined gather fetches a field at an arbitrary displacement
from a site.  It uses the family of calls @kbd{start_general_gather_site},
@kbd{start_general_gather_field},
@kbd{wait_general_gather}, @kbd{cleanup_general_gather}.  It works like
the gather described above except that instead of specifying the
direction you specify a four-component array of integers which is the
relative displacement of the field to be fetched.  It is a bit slower
than a gather defined by @kbd{make_gather}, because it is necessary to
build the neighbor tables with each call to @kbd{start_general_gather}.

Chaos will ensue if you use @kbd{wait_gather()} with a message tag
returned by @kbd{start_general_gather_XXXX()}, or vice-versa.
@kbd{start_general_gather_site()} has the following format:
@example
/* "start_general_gather_site()" starts asynchronous sends and receives
    required to gather neighbors. */ 
msg_tag * start_general_gather_site(offset,size,displacement,parity,dest)
  /* arguments */
  field_offset offset; /* which field? Some member of structure site */
  int size;           /* size in bytes of the field 
                           (eg. sizeof(su3_vector))*/
  int *displacement;  /* displacement to gather from,
                           a four component array */
  int parity;         /* parity of sites whose neighbors we gather.
                           one of EVEN, ODD or EVENANDODD. */
  char ** dest;       /* one of the vectors of pointers */

/* "wait_general_gather()" waits for receives to finish, insuring that
    the data has actually arrived.  The argument is the (msg_tag *) 
    returned by start_general_gather. */ 
void wait_general_gather(msg_tag *mbuf);
@end example

@kbd{start_general_gather_field()} has the following format:
@example
/* "start_general_gather_field()" starts asynchronous sends and receives
    required to gather neighbors. */ 
msg_tag * start_general_gather_field(field,size,displacement,parity,dest)
  /* arguments */
  void *field;        /* which field? A field in field-major order */
  int size;           /* size in bytes per site of the field 
                           (eg. sizeof(su3_vector))*/
  int *displacement;  /* displacement to gather from,
                           a four component array */
  int parity;         /* parity of sites whose neighbors we gather.
                           one of EVEN, ODD or EVENANDODD. */
  char ** dest;       /* one of the vectors of pointers */

/* "wait_general_gather()" waits for receives to finish, insuring that
    the data has actually arrived.  The argument is the (msg_tag *) 
    returned by start_general_gather. */ 
void wait_general_gather(msg_tag *mbuf);

/* "cleanup_general_gather()" frees all the buffers that were
    allocated, @emph{WHICH MEANS THAT THE GATHERED DATA MAY SOON
    DISAPPEAR.}  */ 
void cleanup_general_gather(msg_tag *mbuf);
@end example

This example gathers @kbd{phi} from a site displaced by +1 in the x
direction and -1 in the y direction.
@example
msg_tag *tag; 
site *s; 
int i, disp[4];

disp[XUP] = +1; disp[YUP] = -1; disp[ZUP] = disp[TUP] = 0;

tag = start_general_gather_site( F_OFFSET(phi), sizeof(su3_vector), disp,
                                 EVEN, gen_pt[0] ); /* do other stuff */

wait_general_gather(tag); 
/* gen_pt[0][i] now contains the address of the phi
   vector (or a copy therof) on the site displaced from site i
   by the vector "disp" for all even sites i. */

FOREVENSITES(i,s) @{ 
/* do whatever you want with it here.
   (su3_vector *)(gen_pt[0][i]) is a pointer to phi on
   the other site. */ 
@}

cleanup_general_gather(tag);
@end example

Here is an example of a gather from a field in field-major order:
@example
  su3_vector *tempvec;
  msg_tag *tag;
  tempvec = (su3_vector *)malloc( sites_on_node*sizeof(su3_vector) );
  ...
  FORALLSITES(i,s)@{  tempvec[i] = s->grand; @}
  ...
  tag=start_gather_field( tempvec, sizeof(su3_vector), dir,EVEN,gen_pt[1] );
  ...
  wait_gather(tag);
  ...
  cleanup_gather(tag);
  ...
  free(tempvec);
@end example

  At present the code does not support a strided gather from a field
in field-major order.  This could present a problem.
For example, the gauge links are typically defined as an array of four
@kbd{su3_matrices}, and we typically gather only one of them.  This
won't work with @kbd{start_gather_field}.

  Don't try to chain successive gathers by using
 @kbd{start_gather_field}
     to gather the @kbd{gen_pt fields}.  It will gather the pointers, but not
     what they point to.  This is insidious, because it will work on one
     node as you are testing it, but fail on multiple nodes.

To set up the data structures required by the gather routines,
@kbd{make_nn_gathers()} is called in the setup part of the program.
This must be done @emph{after} the call to @kbd{make_lattice()}.

@node Details of gathers and creating new ones, Distributing sites among nodes, Accessing fields at other sites, Programming with MILC Code
@section Details of gathers and creating new ones
@cindex Details of gathers and creating new ones

(You don't need to read this section the first time through.)

The nearest-neighbor and fixed-displacement gathers are always available
at run time, but a user can make other gathers using the
@kbd{make_gather} routine.  Examples of such gathers are the bit-reverse
and butterfly maps used in FFT's.  The only requirement is that the
gather pattern must correspond to a one-to-one map of sites onto sites.
@kbd{make_gather} speeds up gathers by preparing tables on each node
containing information about what sites must be sent to other nodes or
received from other nodes. The call to this routine is:
@example
#include <comdefs.h> 
int make_gather( function, arg_pointer, inverse, want_even_odd,
                 parity_conserve )
  int (*function)();
  int *arg_pointer;
  int inverse;
  int parity_conserve;
@end example

The @kbd{"function"} argument is a pointer to a function which defines
the mapping. This function must have the following form:
@example
int function( x, y, z, t, arg_pointer, forw_back, xpt, ypt, zpt, tpt)
  int x,y,z,t;
  int *arg_pointer;
  int forw_back;
  int *xpt,*ypt,*zpt,*tpt;
@end example
Here @kbd{x,y,z,t} are the coordinates of the site @emph{RECEIVING} the
data.  @kbd{arg_pointer} is a pointer to a list of integers, which is
passed through to the function from the call to @kbd{make_gather()}.
This provides a mechanism to use the same function for different
gathers.  For example, in setting up nearest neighbor gathers we would
want to specify the direction.  See the examples below.

@kbd{forw_back} is either @strong{FORWARDS} or @strong{BACKWARDS}.  If
it is @strong{FORWARDS}, the function should compute the coordinates of
the site that sends data to @kbd{x,y,z,t}.  If it is @strong{BACKWARDS},
the function should compute the coordinates of the site which gets data
from @kbd{x,y,z,t}.  It is necessary for the function to handle
@strong{BACKWARDS} correctly even if you don't want to set up the
inverse gather (see below).  At the moment, only one-to-one (invertible)
mappings are supported.

@cindex FFT
@cindex Butterflies (FFT)
The @kbd{inverse} argument to @kbd{make_gather()} is one of
@strong{OWN_INVERSE}, @strong{WANT_INVERSE}, or @strong{NO_INVERSE}.  If
it is @strong{OWN_INVERSE}, the mapping is its own inverse.  In other
words, if site @kbd{x1,y1,z1,t1} gets data from @kbd{x2,y2,z2,t2} then
site @kbd{x2,y2,z2,t2} gets data from @kbd{x1,y1,z1,t1}.  Examples of
mappings which are there own inverse are the butterflies in FFT's.  If
@kbd{inverse} is @strong{WANT_INVERSE}, then @kbd{make_gather()} will
make two sets of lists, one for the gather and one for the gather using
the inverse mapping.  If @kbd{inverse} is @strong{NO_INVERSE}, then
only one set of tables is made.

The @kbd{want_even_odd} argument is one of @strong{ALLOW_EVEN_ODD} or
@strong{NO_EVEN_ODD}.  If it is @strong{ALLOW_EVEN_ODD} separate tables
are made for even and odd sites, so that start gather can be called with
parity @strong{EVEN}, @strong{ODD} or @strong{EVENANDODD}.  If it is
@strong{NO_EVEN_ODD}, only one set of tables is made and you can only
call gathers with parity @strong{EVENANDODD}.

The @kbd{parity_conserve} argument to @kbd{make_gather()} is one of
@strong{SAME_PARITY}, @strong{SWITCH_PARITY}, or @strong{SCRAMBLE_PARITY}.
Use @strong{SAME_PARITY} if the gather connects even sites to even sites
and odd sites to odd sites.  Use @strong{SWITCH_PARITY} if the gather
connects even sites to odd sites and vice versa.  Use
@strong{SCRAMBLE_PARITY} if the gather connects some even sites to even
sites and some even sites to odd sites.  If you have specified
@strong{NO_EVEN_ODD} for @kbd{want_even_odd}, then the
@kbd{parity_conserve} argument does nothing.  Otherwise, it is used by
@kbd{make_gather()} to help avoid making redundant lists.

@kbd{make_gather()} returns an integer, which can then be used as the
@kbd{direction} argument to @kbd{start_gather()}.  If an inverse
gather is also requested, its @kbd{direction} will be one more than
the value returned by @kbd{make_gather()}.  In other words, if
@kbd{make_gather()} returns 10, then to gather using the inverse mapping
you would use 11 as the direction argument in start_gather.

Notice that the nearest neighbor gathers do not have their inverse
directions numbered this convention.  Instead, they are sorted so that
@strong{OPP_DIR}(@kbd{direction}) gives the gather using the inverse
mapping.


Now for some examples which we hope will clarify all this.

First, suppose we wish to set up nearest neighbor gathers.  (Ordinarily.
 @kbd{make_comlinks()} already does this for you, but it is a
good example.)  The function which defines the mapping is basically
@kbd{neighbor_coords()}, with a wrapper which fixes up the arguments.
@kbd{arg} should be set to the address of the direction ---
@strong{XUP}, etc.
@example
/* The function which defines the mapping */
void neighbor_temp(x,y,z,t, arg, forw_back, xpt, ypt, zpt, tpt)
  int x,y,z,t;
  int *arg;
  int forw_back;
  int *xpt,*ypt,*zpt,*tpt;
@{
  register int dir; /* local variable */
  dir = *arg;
  if(forw_back==BACKWARDS)dir=OPP_DIR(dir);
  neighbor_coords(x,y,z,t,dir,xpt,ypt,zpt,tpt);
@}

/* Code fragment to set up the gathers */
/* Do this once, in the setup part of the program. */
int xup_dir, xdown_dir;
int temp;
temp = XUP; /* we need the address of XUP */
xup_dir = make_gather( neighbor_temp, &temp, WANT_INVERSE,
                       ALLOW_EVEN_ODD, SWITCH_PARITY);
xdown_dir = xup_dir+1;

/* Now you can start gathers */
start_gather_field( phi, sizeof(su3_vector), xup_dir, EVEN,
                    gen_pt[0] );
/* and use wait_gather, cleanup_gather as always. */
@end example

Again, once it is set up, it works just as before.  Essentially, you are
just defining new @kbd{directions}.  Again, @kbd{make_comlinks()} does
the same thing, except that it arranges the directions so that you can
just use @strong{XUP}, @strong{XDOWN}, etc. as the @kbd{direction}
argument to @kbd{start_gather()}.

A second example is for a gather from a general displacement.  You
might, for example, set up a bunch of these to take care of the link
gathered from the second mearest neighbor in evaluating the plaquette in
the pure gauge code.  In this example, the mapping function needs a list
of four arguments --- the displacement in each of four directions.  Notice
that for this displacement even sites connect to even sites, etc.
@example
/* The function which defines the mapping */
/* arg is a four element array, with the four displacements */
void general_displacement(x,y,z,t, arg, forw_back, xpt, ypt, zpt, tpt)
  int x,y,z,t;
  int *arg;
  int forw_back;
  int *xpt,*ypt,*zpt,*tpt;
@{
    if( forw_back==FORWARDS ) @{ /* add displacement */
       *xpt = (x+nx+arg[0])%nx;
       *ypt = (y+ny+arg[1])%ny;
       *zpt = (z+nz+arg[2])%nz;
       *tpt = (t+nt+arg[3])%nt;
    @}
    else @{ /* subtract displacement */
       *xpt = (x+nx-arg[0])%nx;
       *ypt = (y+ny-arg[1])%ny;
       *zpt = (z+nz-arg[2])%nz;
       *tpt = (t+nt-arg[3])%nt;
    @}
@}

/* Code fragment to set up the gathers */
/* Do this once, in the setup part of the program. */
/* In this example, I set up to gather from displacement -1 in
   the x direction and +1 in the y direction */
int plus_x_minus_y;
int disp[4];
disp[0] = -1;
disp[1] = +1;
disp[2] = 0;
disp[3] = 0;
plus_x_minus_y = make_gather( general_displacement, disp,
                              NO_INVERSE, ALLOW_EVEN_ODD, SAME_PARITY);

/* Now you can start gathers */
start_gather_site( F_OFFSET(link[YUP]), sizeof(su3_matrix), plus_x_minus_y,
                   EVEN, gen_pt[0] );
/* and use wait_gather, cleanup_gather as always. */
@end example

Finally, we would set up an FFT butterfly  roughly as
follows.  Here the function wants two arguments: the direction of the
butterfly and the level.
@example
/* The function which defines the mapping */
/* arg is a two element array, with the direction and level */
void butterfly_map(x,y,z,t, arg, forw_back, xpt, ypt, zpt, tpt)
  int x,y,z,t;
  int *arg;
  int forw_back;
  int *xpt,*ypt,*zpt,*tpt;
@{
int direction,level;
    direction = arg[0];
    level = arg[1];
    /* Rest of code goes here */
@}

/* Code fragment to set up the gathers */
/* Do this once, in the setup part of the program. */
int butterfly_dir[5]; /* for nx=16 */
int args[2];
args[0]=XUP;
for( level=1; level<=4; level++ ) @{
    args[1]=level;
    butterfly_dir[level] = make_gather( butterfly_map, args,
       OWN_INVERSE, NO_EVEN_ODD, SCRAMBLE_PARITY);
@}
/* similarly for y,z,t directions */
@end example

@cindex iPSC-860
@cindex Intel Paragon
@cindex interupts

@node Distributing sites among nodes, Reading and Writing Lattices and Propagators, Details of gathers and creating new ones, Programming with MILC Code
@section Distributing sites among nodes
@cindex Distributing sites among nodes

Four functions are used to determine the distribution of @kbd{sites}
among the parallel @emph{nodes}.

@kbd{setup_layout()} is called once on each node at initialization time,
to do any calculation and set up any static variables that the other
functions may need.  At the time @kbd{setup_layout()} is called the
global variables @kbd{nx,ny,nz} and @kbd{nt} (the lattice dimensions)
are set.  
@sp 1

@kbd{setup_layout()} must initialize the global variables:
@example
sites_on_node, 
even_sites_on_node, 
odd_sites_on_node.
@end example
@sp 1

The following functions are available for node/site reference:
@table @kbd
@item size_t num_sites(int node) 
returns the number of sites on a node
@item int node_number(int x, int y, int z, int t) 
returns the node number on which a site lives.
@item int node_index(int x, int y, int z, int t)
returns the index of the site on the node.
@item int io_node(int node)
returns the I/O node assigned to a node
@item const int *get_logical_dimensions()
returns the machine dimensions as an integer array
@item const int *get_logical_coordinates()
returns the node coordinates when the machine is viewed as a grid of nodes
@item void get_coords(int coords[], int node, int index)
the inverse of node_index and node_number.  Returns coords.
@end table
Thus, the site at @kbd{x,y,z,t} is @kbd{lattice[node_index(x,y,z,t)]}.

These functions may be changed, but
chaos will ensue if they are not consistent. For example, it is a gross
error for the @kbd{node_index} function to return a value larger than or
equal to the value returned by @kbd{num_sites} of the appropriate node.
In fact, @kbd{node_index} must provide a one-to-one mapping of the
coordinates of the @kbd{sites} on one node to the integers from 0 to
@kbd{num_sites(node)-1}.


A good choice of site distribution on nodes will minimize the amount of
communication.  These routines are in @file{layout_XXX.c}. There are
currently several layout strategies to choose from; select one in your
@file{Makefile} (@pxref{Building the MILC Code}).
@table @file
@item  layout_hyper_prime.c
Divides the lattice up into hypercubes by dividing dimensions by the
smallest prime factors.  This is pretty much our standard layout these days.

@item  layout_timeslices.c layout_timeslices_2.c
These routines are now obsolete.  They arranged sites so entire time
slices appeared on each processor.  This was especially efficient for
spatial Fourier transforms.  The same effect can now be obtained with
the @strong{IO_NODE_GEOM} macro or QMP @strong{-qmp_geom} command-line
option.

@item layout_hyper_sl32.c
Version for 32 sublattices, for extended actions.
This version divides the lattice by factors of two in any of the
   four directions.  It prefers to divide the longest dimensions,
   which mimimizes the area of the surfaces.  Similarly, it prefers
   to divide dimensions which have already been divided, thus not
   introducing more off-node directions.
 This requires that the lattice volume be divisible by the number
   of nodes, which is a power of two.
@end table

Below is a completely simple example, which just deals out the sites
among nodes like cards in a deck.  It works, but you would really want
to do much better.
@example
int Num_of_nodes; /* static storage used by these routines */

void setup_layout() @{
  Num_of_nodes = numnodes();
  sites_on_node = nx*ny*nz*nt/Num_of_nodes;
  even_sites_on_node = odd_sites_on_node = sites_on_node/2; 
@}

int node_number(x,y,z,t) 
int x,y,z,t; 
@{
  register int i;
  i = x+nx*(y+ny*(z+nz*t));
  return( i%Num_of_nodes ); 
@}

int node_index(x,y,z,t) 
int x,y,z,t; 
@{
  register int i;
  i = x+nx*(y+ny*(z+nz*t));
  return( i/Num_of_nodes ); 
@}

int num_sites(node) 
int node; 
@{ 
  register int i;
  i = nx*ny*nz*nt;
  if( node< i%Num_of_nodes ) return( i/Num_of_nodes+1 );
  else return( i/Num_of_nodes ); 
@}
@end example

@example
/* utility function for finding coordinates of neighbor */ 
/* x,y,z,t are the coordinates of some site, and x2p... are
   pointers.  *x2p... will be set to the coordinates of the
   neighbor site at direction "dir".

void neighbor_coords( x,y,z,t,dir, x2p,y2p,z2p,t2p)
  int x,y,z,t,dir; /* coordinates of site, and direction (eg XUP) */
  int *x2p,*y2p,*z2p,*t2p;
@end example

@node Reading and Writing Lattices and Propagators, Random numbers, Distributing sites among nodes, Programming with MILC Code
@section Reading and writing lattices and propagators
@cindex  Reading and writing lattices and propagators

The MILC code supports a MILC-standard single-precision binary gauge
configuration (``lattice'') file format.  This format includes a
companion ASCII metadata ``info'' file with standardized fields to
describe the parameters that went into creating the configuration.
The code also reads gauge configurations files in NERSC, ILDG, SciDAC,
and Fermilab formats and writes files in NERSC, ILDG, and SciDAC
formats.  For MILC, ILDG, and SciDAC formats, it supports reading and
writing either serially through node 0 or in parallel to and from a
single file.  There is also an historic but seldom-used provision for
reading and writing ASCII gauge configuration files.  Through SciDAC
QIO the MILC code supports parallel reading and writing of multiple
partitioned SciDAC file formats.

The MILC code supports the USQCD Dirac and staggered propagator file
formats through QIO.  It also supports reading and writing two
standard Fermilab propagator file formats.

Input and output types are set at run time.
Here is a summary of the I/O choices for gauge configuration files:

@table @kbd
@item  reload_ascii
@item   reload_serial 
   Binary read by node 0.  File format is autodetected.
@item   reload_parallel
  Binary read by all nodes.  Only where hardware is available.
@item   save_ascii

@item   save_serial 
     Binary through node 0 - standard index order
@item   save_parallel
    Binary parallel - standard index order.  Only where hardware is available.
@item   save_checkpoint 
    Binary parallel - node dump order
@item   save_serial_archive
    NERSC Gauge Connection format.  For the moment a parallel version
    is not available.
@item   save_serial_scidac
@item   save_parallel_scidac
@item   save_serial_ildg
    SciDAC single-file format and ILDG-compatible format.
@item   save_partition_scidac
@item   save_partition_ildg
    SciDAC partition file format and ILDG-compatible partition file format.
@item   save_multifile_scidac
@item   save_multifile_ildg
    SciDAC multifile format. (Node dump order, intended for temporary
    storage, rather than archiving.)
@end table

For Dirac propagators the old MILC-standard format has been abandoned
in favor of the USQCD and Fermilab formats.  Here is a summary of
Dirac propagator I/O choices:

@table @kbd
@item  reload_ascii_wprop
@item   reload_serial_wprop 
   Binary read by node 0.  File format is autodetected.
@item   reload_parallel_wprop
  Binary read by all nodes.  Only where hardware is available.
@item   save_ascii_wprop
@item   save_serial_scidac_wprop
     Binary through node 0 - standard index order
@item   save_parallel_scidac_wprop
    Binary parallel - standard index order.  Only where hardware is available.
@item   save_partfile_scidac_wprop
    Binary parallel with multiple files.
@item   save_multifile_scidac_wprop
    Binary parallel with multiple files - node dump order
@item   save_serial_fm_wprop
@item   save_serial_fm_sc_wprop
    Fermilab formats
@end table

For staggered propagators the code supports a MILC-standard format as
well as the USQCD formats and the Fermilab format.  Here is a summary
of Dirac propagator I/O choices:

@table @kbd
@item  reload_ascii_ksprop
@item   reload_serial_ksprop 
   Binary read by node 0.  File format is autodetected.
@item   reload_parallel_ksprop
  Binary read by all nodes.  Only where hardware is available.
@item   save_ascii_ksprop
@item   save_serial_ksprop
     Binary through node 0 - standard index order
@item   save_serial_scidac_ksprop
    Binary through node 0 - standard index order
@item   save_parallel_scidac_ksprop
    Binary parallel - standard index order.  Only where hardware is available.
@item   save_partfile_scidac_ksprop
    Binary parallel with multiple files.
@item   save_multifile_scidac_ksprop
    Binary parallel with multiple files - node dump order
@item   save_serial_fm_ksprop
     Fermilab format.
@end table

To print out variables for debugging purposes, the library routines
@kbd{dumpvec}, @kbd{dumpmat} and @kbd{dump_wvec} (for SU(3) vectors,
SU(3) matrices and Wilson vectors) are quite useful:

@example

          FORALLSITES(i,s) @{
        if(magsq_wvec(&(s->psi)) > 1.e-10) @{
        printf("%d %d %d %d\n", s->x, s->y, s->z,s->t);
        printf("psi\n");
        dump_wvec(&(s->psi));
        @}
    @}
@end example


@node Random numbers, A Sample Applications Directory, Reading and Writing Lattices and Propagators, Programming with MILC Code
@section Random numbers
@cindex Random numbers

The random number generator is the exclusive-OR of a 127 bit feedback
shift register and a 32 bit integer congruence generator.  It is
supposed to be machine independent.  Each node or site uses a
different multipler in the congruence generator and different initial
state in the shift register, so all are generating different sequences
of numbers.  If @strong{SITERAND} is defined, which has become
standard practice in our code, each lattice site has its own random
number generator state.  This takes extra storage, but means that the
results of the program will be independent of the number of nodes or
the distribution of the sites among the nodes.  The random number
routine is @kbd{generic/ranstuff.c}.


@node A Sample Applications Directory, Bugs and features, Random numbers, Programming with MILC Code
@section A Sample Applications Directory
@cindex A Sample Applications Directory

An example of the files which make up an application are listed here.
This list depends very much on which application of the program
(Kogut-Susskind/Wilson?  Thermodynamics/Spectrum?) is being built. The
listing here is for the pure MILC code (non-SciDAC) compilation of the
@kbd{su3_spectrum} target for the 2+1 asqtad action in the
@kbd{ks_imp_dyn} application.
@table @file
@item Make_template:
	Contains instructions for compiling and linking. 
	Three routines you can make, "su3_rmd", "su3_phi" and "su3_hmc",
	 are programs for the R algorithm, the phi algorithm, or
        the hybrid Monte Carlo algorithm.
@end table
@sp 1

@strong{SOME HIGH LEVEL ROUTINES:}
@table @file
@item control.c
    main program
@item ../generic_ks/ranmom.c 
    generate Gaussian random momenta for the gauge fields
@item ../generic_ks/grsource_imp.c
    generate Gaussian random pseudofermion field
@item setup.c
    interpret parameter input
@item update.c
    top-level molecular dynamics
@item update_h.c
    update the gauge momenta
@item update_u.c
    update the gauge fields
@end table
@sp 1

@strong{HEADERS}
@table @file
@item defines.h
    Required. Local macros common to all targets in an application.
    SITERAND should be defined here.  Other macros that are independent of
    the site structure can also be defined here.
@item gauge_action.h
    Defines the gauge action
@item ks_imp_includes.h
    Function prototypes for all files in the @kbd{ks_imp_dyn} application directory.
@item lattice.h
    Required. Defines the site structure and global variables.
@item lattice_qdp.h
    Required. Used only for QDP support.
@item params.h
    Required. Defines the parameter structure used in @kbd{setup.c} for passing
    input parameters to the nodes.
@item quark_action.h
    Defines the quark action
@end table
@sp 1

@strong{LOWER LEVEL ROUTINES}
@table @file
@item gauge_info.c
      Print gauge info to file.
@item ../generic/ape_smear.c
      APE smearing
@item ../generic/check_unitarity.c
      Check unitarity
@item ../generic/com_qmp.c
      Communications
@item ../generic/d_plaq4.c 
      Plaquette
@item ../generic/field_utilities.c
      Creation and destruction of fields
@item ../generic/file_types_milc_usqcd.c
      USQCD file type translation
@item ../generic/gauge_force_imp.c
      Gauge force
@item ../generic/gauge_stuff.c
      Construction of gauge force.
@item ../generic/gauge_utilities.c
      Gauge field creation and destruction
@item ../generic/gaugefix2.c 
      Gauge fixing
@item ../generic/general_staple.c 
      General staple calculation
@item ../generic/io_ansi.c
      Interface for ANSI I/O
@item ../generic/io_detect.c
      File type recognition
@item ../generic/io_helpers.c
      Top level lattice I/O
@item ../generic/io_lat4.c 
      Gauge field I/O
@item ../generic/io_lat_utils.c
      Gauge field I/O
@item ../generic/io_scidac.c
      SciDAC file I/O wrapper
@item ../generic/io_scidac_types.c
      File type recognition
@item ../generic/layout_hyper_prime.c
      Lattice layout
@item ../generic/make_lattice.c
      Lattice creation
@item ../generic/momentum_twist.c 
      Momentum twist insertion, removal
@item ../generic/nersc_cksum.c 
      NERSC-type checksum
@item ../generic/path_product.c
      Paralle transport along path
@item ../generic/ploop3.c
      Polyakov loop
@item ../generic/project_su3_hit.c
      SU(3) projection
@item ../generic/ranstuff.c
      Random number generator
@item ../generic/remap_stdio_from_args.c
      Standard I/O remapping
@item ../generic/report_invert_status.c
      Status report
@item ../generic/reunitarize2.c
      Reunitarization of gauge links
@item ../generic/show_generic_opts.c
      List options turned on
@item ../generic_ks/d_congrad5_fn.c
      Conjugate gradient inverter - top level
@item ../generic_ks/d_congrad5_fn_milc.c
      MILC coded inverter - single mass
@item ../generic_ks/d_congrad5_two_src.c
      Two-source inverter
@item ../generic_ks/d_congrad_opt.c
      Inverter support
@item ../generic_ks/dslash_fn_dblstore.c 
      Dslash
@item ../generic_ks/f_meas.c
      Scalar operators, such as psi-bar-psi
@item ../generic_ks/fermion_force_asqtad.c 
      Fermion force
@item ../generic_ks/fermion_force_fn_multi.c 
      Fermion force
@item ../generic_ks/fermion_force_multi.c 
      Fermion force for multiple shifts
@item ../generic_ks/fermion_links.c 
      Fermion link creation/destruction
@item ../generic_ks/fermion_links_fn_load_milc.c
      Asqtad link fattening
@item ../generic_ks/fermion_links_fn_twist_milc.c
      Boundary twist
@item ../generic_ks/fermion_links_from_site.c
      Wrapper for creating links from gauge field
@item ../generic_ks/fermion_links_milc.c 
      Fermion link fattening
@item ../generic_ks/ff_opt.c  
      Fermion force optimization
@item ../generic_ks/fn_links_milc.c 
      Creation/destruction utilities for fermion links
@item ../generic_ks/ks_action_paths.c 
      Link fattening path
@item ../generic_ks/ks_invert.c 
      Wrapper for staggered inversion
@item ../generic_ks/ks_multicg.c 
      Multimass inverter - top level
@item ../generic_ks/ks_multicg_offset.c 
      Multimass inverter
@item ../generic_ks/mat_invert.c 
      Inverter for spectrum
@item ../generic_ks/path_transport.c
      Parallel transport
@item ../generic_ks/rephase.c
      Insert/remove KS phases
@item ../generic_ks/show_generic_ks_md_opts.c
      Show options in force
@item ../generic_ks/show_generic_ks_opts.c 
      Show options in force
@item ../generic_ks/show_hisq_force_opts.c
      Show options in force
@sp 1
@end table
@sp 1

@strong{LIBRARIES}
@table @file
@item complex.1.a
	complex number operations.  See section on "Library routines".
@item su3.1.a
	3x3 matrix and 3 component vector operations.  See "utility
	subroutines".
@end table


@node Bugs and features, , A Sample Applications Directory, Programming with MILC Code
@section Bugs and features
@cindex Bugs and features
@cindex doubleword boundaries


Some variants of the version 4 MILC code had library routines and
applications for doing SU(2) gauge theory. This code has not been
translated into version 7 conventions.

The MILC code is hard-wired for four dimensional simulations.
Three and five dimensional variants are believed to exist, but are not
maintained by us.

For some unknown reason, lost in the mists of time,
 the MILC code uses the opposite convention for the projectors
@tex
$$S = \sum_x \bar \psi(x) \psi(x) - \kappa(
 \sum_\mu \bar \psi(x) (1+\gamma_\mu)U_\mu(x) \psi(x+\mu) +
 \sum_\mu \bar \psi(x+\mu) (1-\gamma_\mu)U_\mu^\dagger(x) \psi(x)) $$
@end tex
rather than the more standard convention
@tex
$$ S = \sum_x \bar \psi(x) \psi(x) - \kappa(
 \sum_\mu \bar \psi(x) (1-\gamma_\mu)U_\mu(x) \psi(x+\mu) ) +
 \sum_\mu \bar \psi(x+\mu) (1+\gamma_\mu)U_\mu^\dagger(x) \psi(x) ) $$
@end tex
Furthermore, the sign convention for 
@tex $\gamma_2$ 
@end tex 
is minus the standard convention.  Thus the MILC spinor fields differ
from the ``conventional'' ones by a gamma-5 multiplication
@tex $\psi_{MILC}= \gamma_5 \psi_{usual}$. 
@end tex
and a reflection in the x-z plane.
  MILC uses Weyl-basis (gamma-5 diagonal)
 Dirac matrices.

@node Writing Your Own Application, Documentation for Specific Applications, Programming with MILC Code, Top
@chapter Writing Your Own Application
@cindex Writing Your Own Application

Each application typically uses at least four header files,
@kbd{APP_includes.h}, @kbd{lattice.h}, @kbd{defines.h} and
@kbd{params.h}. The first contains definitions of all the routines in
the application directory; the second contains a list of global macros
and global definitions and a specification of the @kbd{site} structure
(all the variables living on the sites of your lattice); the third
contains local macros common to all targets in an application; and the
last defines the parameter structure used for passing input parameters
to the nodes.  The application directory also has its own makefile,
@kbd{Make_template}, which contains all the (machine-independent)
dependencies necessary for your applications.

It is customary to put the @kbd{main()} routine in a file which begins with
the word ``control'' (e.g. @kbd{control_clov_p.c}). Beyond that, you are
on your own. If you poke around, you may find a routine similar
to the one you want to write, which you can modify. A typical application
reads in a lattice 
reads in some input parameters,
does things to variables on every site,
moves information from site to site (@pxref{Accessing fields at other sites})
and
writes out a lattice at the end.

One problem is how to read in global parameters (like a coupling
or the number of iterations of a process). This is done in a file with
a name similar to @kbd{setup.c}. To read in a global variable and
broadcast it to all the nodes, you must first edit @kbd{lattice.h}
adding the variable to the list of globals
 (in the long set of @kbd{EXTERN}'s
at the top), and edit the @kbd{params.h} file to
add it to the @kbd{params} structure. Next, in @kbd{setup.c},
add a set of lines to prompt for the input,
and find the place where the @kbd{params} structure is initialized or read.
Add your variable once to each list, to put it into @kbd{par_buf},
and then to extract it. Good luck!

@node Documentation for Specific Applications, Concept Index, Writing Your Own Application, Top
@chapter Documentation for Specific Applications
@cindex Documentation for Specific Applications

We provide, here, documentation for some of the applications.

@menu
* clover_invert2::
* ks_spectrum::
@end menu

@node clover_invert2, ks_spectrum, Documentation for Specific Applications, Documentation for Specific Applications
@section clover_invert2
@cindex clover_invert2

This code generates Dirac clover and naive propagators from a wide
variety of sources and ties them together to form meson and baryon
correlators.  With extended sources, the code also computes
correlators needed fo three-point and higher functions. The code is
designed to be flexible in specifying source interpolating operators,
propagators, sink interpolating operators, and the combinations to
form hadrons.  In broad terms a ``meson'' is contructed by tying
together quark and antiquark propagators.  These quark propagators are
generated in a succession of steps.  First a base source is defined.
A base source can then be modified by applying a sequence of source
operators to complete the definition of the source interpolating
operator.  The propagator starts from the source and propagates to any
sink location.  At the sink a variety of operators can be applied to
form the completed quark propagator.  They are then used to form the
hadrons.

Because of the high degree of flexibility in the code, the parameter
input file is rather complex and highly structured.  We explain
the construction of the parameter input file in some detail here.
Examples can be found in the directory
@strong{clover_invert2/test/*.sample-in}.

The parameter input file consists of the following required parts in
this order:

@menu
* Geometry etc. specification::
* Gauge field specification::
* Inversion control::
* Base sources::
* Modified sources::
* Propagator description::
* Quark description::
* Combine description::
* Hadron description::
@end menu

This complete set of commands can be repeated indefinitely to form a
chain of consecutive computations.

For human readability, comments on lines beginning with hash (#) are
allowed in the parameter input file.  Blank lines are also permitted.

@sp 1

@node Geometry etc. specification, Gauge field specification, clover_invert2, clover_invert2
@subsection Geometry etc. specification
@cindex  Geometry etc. specification

@example
prompt <int>
@end example

Here, valid values of <int> are 0, 1, 2.  The value 0 takes parameters
from stdin without prompting.  The value 1 takes parameters from stdin
with stdout prompts.  The value 2 is for proofreading.  The parameter
input file is scanned but no computations take place.

@example
nx <int>
ny <int>
nz <int>
nt <int>
@end example

These are the lattice dimensions.

@example
node_geometry <int[4]>
@end example

If the code is compiled with the macro @strong{FIX_NODE_GEOM}, the
above line is required.  Otherwise, it should be omitted.  It sets up a
mesh view of the machine based on the machine x y z t dimensions.  The
lattice nx ny nz nt dimensions are divided according to these values.
The product of the int[4] values must equal the number of cores to be
used.

@example
ionode_geometry <int[4]>
@end example

If the code is compiled with both @strong{FIX_NODE_GEOM} and
@strong{FIX_IONODE_GEOM} defined, then the above line is required.  It
sets I/O partitions for SciDAC partfile operation.  The values 1 1 1 1
put the entire machine into one I/O partition, so only one partfile is
written/read.  Copying the values in node_geometry puts each core in
its own I/O partition, so resulting in as many partfiles read/written
as there are cores.  Each value in ionode_geometry must be a divisor
of the corresponding value in node_geometry.

@example
iseed <int>
@end example

This line sets the random number seed.  It is needed for random wall sources.

@example
job_id <char[]>
@end example

The Job ID is copied into the metadata in the output correlator files.

@node Gauge field specification, Inversion control, Geometry etc. specification, clover_invert2
@subsection Gauge field specification
@cindex  Gauge field specification

The gauge field specification has the following required components:

@menu
* Gauge field input::
* Tadpole factor and gauge fixing::
* Gauge field output::
* APE smearing::
* Coordinate origin::
@end menu

@node Gauge field input, Tadpole factor and gauge fixing, Gauge field specification, Gauge field specification, 
@subsubsection Gauge field input
@cindex  Gauge field input

The following choices are available.  Use only one of these.

@example
reload_serial <char[]> |
reload_parallal <char[]> |
reload_ascii <char[]> |
fresh |
continue
@end example

The <char[]> string is the path to the file.  The file type is
detected automatically.  Supported binary file types include MILCv5,
SciDAC (USQCD), ILDG, NERSC, and FNAL-style.  If the file is in SciDAC
format in multiple parts with volume extensions @strong{volnnnnn}, the
path should omit the volume extension.

With serial reading only the I/O nodes read the file. With SciDAC
singlefile format, there is only one I/O node, namely the master node.
With SciDAC partfile format, each I/O partition resulting from the
@strong{ionode_geometry} specification has one I/O node and it reads
its own partfiles serially.  With parallel reading all nodes read the
file.  Currently, parallel reading is not supported for partfiles, but
future versions of SciDAC QIO may do so.

Provision is made for reading a file in ASCII format, but this mode is
never used for production running.  

The @strong{fresh} command specifies a gauge field consisting of unit
matrices.  The @strong{continue} command is used when several
computations are chained together.  It retains the previously loaded
gauge configuration.

@node Tadpole factor and gauge fixing, Gauge field output, Gauge field input, Gauge field specification
@subsubsection Tadpole factor and gauge fixing
@cindex  Tadpole factor and gauge fixing

@example
u0 <float>
@end example

This sets the tadpole factor.

@example
coulomb_gauge_fix |
no_gauge_fix
@end example

This line specifies whether to fix to Coulomb gauge or not.  Choose one of these.

@node Gauge field output, APE smearing, Gauge field input, Gauge field specification
@subsubsection Gauge field output
@cindex  Gauge field output

@example
forget |
save_ascii <char[]> |
save_serial <char[]> |
save_parallel <char[]> |
save_checkpoint <char[]> |
save_serial_fm <char[]> |
save_serial_scidac <char[]> |
save_parallel_scidac <char[]> |
save_multifile_scidac <char[]> |
save_partfile_scidac <char[]> |
save_serial_ildg <char[]> |
save_parallel_ildg <char[]> |
save_multifile_ildg <char[]> |
save_partfile_ildg <char[]>
@end example

Choose one of these for saving the gauge configuration to a file.  The
command @strong{forget} does not save it.  Otherwise, the <char[]>
specifies the path to the file to be created.

The @strong{save_serial}, @strong{save_parallel}, and
@strong{save_checkpoint} commands save the file in MILCv5 format.  The
mode of writing is either serial (only the master node writes the
file), parallel (all nodes write to a single file), and checkpoint
(each node writes its own data to a separate file.)  The checkpoint
format is deprecated.  It is being replaced by the corresponding
SciDAC partfile format.

The @strong{save_serial_fm} command saves the file in Fermilab format.

The various @strong{scidac} and @strong{ildg} modes require
compilation with QIO and QMP.  The @strong{serial_scidac} mode writes
to a single file through a single node.  The @strong{parallel_scidac}
mode writes to a single file with all nodes writing.  The
@strong{partfile_scidac} mode writes multiple partfiles, one file per
I/O partition.  The @strong{multifile_scidac} mode writes a separate
file for each node.

The @strong{ildg} mode generates an ILDG-compatible file.  It
requires specifying the ILDG logical file name (LFN) on a separate
line immediately following the @strong{save_XXX_ildg} line:

@example
ILDG_LFN <char[]>
@end example

@node APE smearing, Coordinate origin, Gauge field output, Gauge field specification
@subsubsection APE smearing
@cindex  APE smearing

@example
staple_weight <float>
ape_iter <int>
@end example

Provision is made to smooth the input gauge field with the specified
number of APE iterations using the specified weight.  These lines are
required even if smearing is not desired.  In that case use zeros for
each value.

@node Coordinate origin, Inversion control, APE smearing, Gauge field specification
@subsubsection Coordinate origin
@cindex  coordinate origin

Boundary conditions and momentum (``boundary'') twists for the Dirac
operator depend on the coordinate origin, which establishes which
gauge links are on the boundary.  It is common practice to boost
statistics by computing the same hadron correlator from several
origins on the same gauge field configuration.  It may be desirable to
guarantee that the result is equivalent to applying a straight
coordinate translation to the gauge field configuration and then
computing propagators from an unshifted origin.  The coordinate origin
parameter does this

@example
coordinate_origin <int[4]>
@end example

The order of coordinates is, as usual, x, y, z, and t.

@node Inversion control, Base sources, Gauge field specification, clover_invert2, 
@subsection Inversion control
@cindex  Inversion control

@example
max_cg_iterations <int>
max_cg_restarts <int>
@end example

These lines specify the maximum number of CG iterations allowed before
restarting and the maximum number of restarts.  Thus the overall
maximum number of iterations is the product of these two.  Further
inversion control is given below for each propagator.

@node Base sources, Modified sources, Inversion control, clover_invert2, 
@subsection Base sources
@cindex  Base sources

Here we describe the specification of the base sources.  They are
elementary and can be used by themselves or made more elaborate with
modifications.

The definition starts by specifying the number of base sources,
followed by that many stanzas describing the sources.  Here we have
used a hash (#) to introduce comments.  Choices for the
@strong{[source_stanza]} are explained below.

@example
number_of_base_sources <int = n>

# source 0

[source_stanza]

# source 1

[source_stanza]

etc.

# source n-1

[source_stanza]
@end example

Each @strong{[source_stanza]} has the following structure.  Items in
brackets [] are explained below.

@example
<char[] source_name>
subset <full|corner>
[source_parameters]
[optional scale factor]
source_label <char[]>
[save_source_specification]
@end example

The @strong{source_name} can specify a complex field, a color vector
field, or a dirac field (usually defined on a single time slice).  For
each source type the @strong{[source_parameters]} are specified as
explained in more detail below.  For a Dirac propagator, the spin and
color content is completed by multiplying an appropriate spin-color
unit vector by the specified complex field.  The possible choices and
their @strong{[source_parameters]} are described in detail below.

The @strong{subset} mask is applied to the base source and to any of
its modifications.  The choices are @strong{corner} and @strong{full}.
With @strong{corner}, after a source is constructed, the values on all
sites are set to zero except on the corners of each 2^4 hypercube.
With @strong{full}, no mask is applied.  This mask is useful for
naive and staggered fermion sources.

The @strong{[optional_scale_factor]} has the form
@example
scale_factor <float>
@end example

This line is read only if the code is compiled with the macro
@strong{SCALE_PROP} defined.  It multiplies the source by this factor,
and then divides the resulting hadron correlators by this factor to
compensate, which should result in no visible modification of the
result.  It is useful to prevent exponent underflow in computing
propagators for especially heavy quarks.  At the moment this factor is
applied only to Dirac field sources.

The @strong{source_label} should be only a couple characters long,
since it is used in constructing a name for the hadron correlator.

Provision is made for saving the source to a file.  Currently, this
happens only if the source is actually used to calculate a propagator.
The @strong{[save_source_specification]} has the syntax

@example
forget_source |
save_serial_scidac_ks_source <char[]> |
save_multifile_scidac_ks_source <char[]> |
save_partfile_scidac_ks_source <char[]> |
save_serial_scidac_w_source <char[]> |
save_multifile_scidac_w_source <char[]> |
save_partfile_scidac_w_source <char[]>
@end example

These choices follow the same pattern as the choices for saving the
gauge configuration, except they are more limited.

We turn now to a description of the various source types and their
parameters.  Complex field source names can be any of the following:

@menu
* complex_field::
* complex_field_fm::
* corner_wall::
* even_wall::
* evenandodd_wall::
* evenminusodd_wall::
* gaussian::
* point::
* random_complex_wall::
* wavefunction::
@end menu

A color vector field serves as the complete source for a naive
propagator (which begins its life as a staggered propagator).  For a
clover propagator, the spin content is completed by multipying the
color field by an appropriate unit spin vector.  Color vector field
names are any of the following:

@menu
* random_color_wall::
* vector_field::
* vector_field_fm::
* vector_propagator_file::
@end menu

Dirac field source names are any of the following:

@menu
* dirac_field::
* dirac_field_fm::
* dirac_propagator_file::
@end menu

@node complex_field, complex_field_fm, Base sources, Base sources
@subsubsection complex_field
@cindex  complex_field

@example
origin <int[4]>
load_source <char[] = file_name>
momentum <int[3]>
@end example

These are the @strong{[source_parameters]} for the
@strong{complex_field} source.  This source is read from a file in
SciDAC format.  The source must be on a single time slice.  The
placement of the source is controlled by the file, which must agree
with the source origin on the @strong{origin} line.  (Typically, the
source file was created in a previous calculation.  In that case the
origin should match the value in that calculation.  If that
calculation required only a value for the time slice coordinate
@strong{t0}, then use the origin @strong{0 0 0 t0}).  The
@strong{momentum} line specifies a plane wave factor that can modify
the source.  The integers specify the x, y, and z components of the
momentum in units of 2 pi/nx, 2 pi/ny, and 2 pi/nz, respectively.

@node complex_field_fm, corner_wall, complex_field, Base sources
@subsubsection complex_field_fm
@cindex  complex_field_fm

@example
origin <int[4]>
load_source <char[] = file_name>
momentum <int[3]>
@end example

This source is similar to the @strong{complex_field} source, except
the binary file is in Fermilab format.

@node corner_wall, even_wall, complex_field_fm, Base sources
@subsubsection corner_wall
@cindex corner_wall

@example
t0 <int>
@end example

On the specified time slice this source is one at the origin of each
2^3 cube and zero elsewhere.

@node even_wall, evenandodd_wall, corner_wall, Base sources
@subsubsection even_wall
@cindex even_wall

@example
t0 <int>
@end example

On the specified time slice this source is one on all even sites,
i.e. with (x + y + z) even.

@node evenandodd_wall, evenminusodd_wall, even_wall, Base sources
@subsubsection evenandodd_wall
@cindex evenandodd_wall

@example
t0 <int>
@end example

On the specified time slice this source is one on all sites.

@node evenminusodd_wall, gaussian, evenandodd_wall, Base sources
@subsubsection evenminusodd_wall
@cindex evenminusodd_wall

@example
t0 <int>
@end example

On the specified time slice this source is one on even sites and minus
one on odd sites.

@node gaussian, point, evenminusodd_wall, Base sources
@subsubsection gaussian_wall
@cindex gaussian_wall

@example
origin <int[4]>
r0 <float>
@end example

A Gaussian profile source centered at the origin, specified by the
integers x, y, z, t.  This source has the value
@tex $\exp[-(r/r0)^2]$
@end tex
where r is the Cartesian displacement from x, y, z an the fixed time
slice t.  This is a non-gauge-covariant source, so it is common to use
it only with Coulomb gauge fixing.

@node point, random_complex_wall, gaussian, Base sources
@subsubsection point
@cindex  point source

@example
origin <int[4]>
@end example

A point source of strength one at the origin specified by the integers
x, y, z, and t.

@node random_complex_wall, wavefunction, point, Base sources
@subsection random_complex_wall
@cindex random_complex source

@example
t0 <int>
momentum <int[3]>
@end example

A random complex field of unit magnitude on the time slice
@strong{t0}.  The same random source is used for each of three colors
and, in the Dirac case, each of four spins.  The Fourier phase factor
specified by the three integer momentum components multiplies the source.

@node wavefunction, random_color_wall, random_complex_wall, Base sources
@subsubsection wavefunction
@cindex  wavefunction source

@example
origin <int[4]>
load_source <char[] = file_name>
a <float>
momentum <int[3]>
@end example

Reads an ASCII wavefunction specified in physical coordinates (fm) and
converts it to lattice coordinates for lattice spacing @strong{a}.
The wavefunction is centered at the @strong{origin}, specified by four
integers x, y, z, and t.  A Fourier phase factor specified by the
three integer momentum components multiplies the wave function.


@node random_color_wall, vector_field, wavefunction, Base sources
@subsubsection random_color_wall
@cindex  random_color_wall source

@example
t0 <int>
ncolor <int>
momentum <int[3]>
@end example

Generates @strong{ncolor} random color vector fields on the time slice
@strong{t0}.  For a Dirac source the same random source is used for
each of the four associated spins. A Fourier phase factor specified by
the three integer momentum components multiplies the wave function.

@node vector_field, vector_field_fm, random_color_wall, Base sources
@subsubsection vector_field
@cindex  vector_field source

@example
origin <int[4]>
load_source <char[] = file_name>
ncolor <int>
momentum <int[3]>
@end example

Reads @strong{ncolor} color vector sources from a file in SciDAC
format.  The oritin @strong{origin} must match the origin in the
file. (Typically, the source file was created in a previous
calculation.  In that case the origin should match the value in that
calculation.  If that calculation required only a value for the time
slice coordinate @strong{t0}, then use the origin @strong{0 0 0
t0}). A Fourier phase factor specified by the three integer momentum
components multiplies the wave function.

@node vector_field_fm, vector_propagator_file, vector_field, Base sources
@subsubsection vector_field_fm
@cindex  vector_field_fm source

@example
origin <int[4]>
load_source <char[] = file_name>
ncolor <int>
momentum <int[3]>
@end example

Reads @strong{ncolor} color vector sources from a file in Fermilab
format.  The origin @strong{origin} must match the origin in the
file.  (See the note for @strong{vector_field} for further
explanation.) A Fourier phase factor specified by the three integer
momentum components multiplies the wave function.

@node vector_propagator_file, dirac_field, vector_field_fm, Base sources
@subsubsection vector_propagator_file
@cindex  vector_propagator_file source

@example
ncolor <int>
@end example

Our USQCD-formatted propagator files include the source fields with
the solution fields.  In this case the source field is to be taken
from the file specified in the propagator stanza below.

@node dirac_field, dirac_field_fm, vector_propagator_file, Base sources
@subsubsection dirac_field
@cindex  dirac_field source

@example
origin <int[4]>
load_source <char[] = file_name>
nsource <int>
momentum <int[3]>
@end example

Reads @strong{nsource} Dirac field sources from a file in SciDAC
format.  The origin @strong{origin} must match the origin in the
file. (See the note for @strong{vector_field} for further
explanation.)  A Fourier phase factor specified by the three integer
momentum components multiplies the wave function.

The @strong{nsource} Counts colors times spins, so it should be 12 for
a conventional Dirac source.

@node dirac_field_fm, dirac_propagator_file, dirac_field, Base sources
@subsubsection dirac_field_fm
@cindex  dirac_field_fm source

@example
origin <int[4]>
load_source <char[] = file_name>
momentum <int[3]>
@end example

Reads exactly 12 Dirac field sources from a file in FNAL format.  The
origin @strong{origin} must match the origin in the file. (See the
note for @strong{vector_field} for further explanation.)  A Fourier
phase factor specified by the three integer momentum components
multiplies the wave function.

@node dirac_propagator_file, Modified sources, dirac_field_fm, Base sources
@subsubsection dirac_propagator_file
@cindex  dirac_propagator_file source

@example
nsource <int>
@end example

Our USQCD-formatted propagator files include the source fields with
the solution fields.  In this case the source field is to be taken
from the file specified in the propagator stanza below.  The number of
source to use is specified by @strong{nsource}.


@node Modified sources, Propagator description, Base sources,  clover_invert2, 
@subsection Modified sources
@cindex  Modified sources

The base sources described above can be used directly for computing
propagators, but they can also be modified.  The modifications are
defined by a sequence of operators acting on the base sources and on
the previously modified sources.  The same operators are used at the
propagator sink, as described below.  The only difference is that the
source operators act on only the source time slice, whereas the sink
operators act on all sink time slices.

The operator definition starts by specifying the number of modified
sources, followed by that many stanzas describing the
modifications. Choices for the @strong{[operator_stanza]} are
explained below.

At present all sources are referred to by integers.  The enumeration
of the sources is consecutive from zero, starting with the base
sources and continuing with the modified sources.  Thus after n base
sources the first modified source is source number n.

@example
number_of_modified_sources <int =  m>

# source n

[operator_stanza]

# source n+1

[operator_stanza]

etc.

# source n+m-1

[operator_stanza]
@end example

Each @strong{[operator_stanza]} has the following structure.  Items in
brackets [] are explained below.

@example
<char[] operator_name>
source <int>
[operator_parameters]
op_label <char[]>
[save_source_specification]
@end example

Choices for @strong{operator_name} and the corresponding
@strong{[operator_parameteers]} are described in detail below.

The @strong{source} line specifies the previous source upon which the
operator acts.  This source can be either a previously defined base
source or modified source.

The @strong{op_label} and @strong{[save_source_specification]} are
the same as for the base sources.  Again, in the current code version,
a request to save a source is ignored unless the source is also used
to construct a propagator.

Here are possible choices for the @strong{[operator_stanza]}.
As with the base sources, each has its specific set of parameters.

The first set of operators specify convolutions with a complex field,
which can be applied to both staggered and Dirac fiels:

@menu
* complex_field operator::
* complex_field_fm operator::
* evenandodd_wall operator::
* gaussian operator::
* identity operator::
* wavefunction operator::
@end menu

The second set apply covariant smearing and derivatives, which can
also be applied to both staggered and Dirac fiels:

@menu
* covariant_gaussian operator::
* deriv1 operator::
* deriv2_D operator::
* deriv2_B operator::
* deriv3_A operator::
* fat_covariant_gaussian operator::
* hop operator::
@end menu

The third set is peculiar to staggered fermions:

@menu
* funnywall1 operator::
* funnywall2 operator::
* ks_inverse operator::
* spin_taste operator::
* ext_src_ks operator::
@end menu

The fourth set is used only with Dirac fermions:

@menu
* rotate_3D operator::
* dirac_inverse operator::
* gamma operator::
* ks_gamma operator::
* ks_gamma_inv operator::
* ext_src_dirac operator::
@end menu

The fifth set is common to both Dirac and staggered fermions:

@menu
* project_t_slice operator::
* momentum operator::
* modulation operator::
@end menu

Here are the operators that do convolutions with a complex field:

@node complex_field operator, complex_field_fm operator, Modified sources, Modified sources
@subsubsection complex_field operator
@cindex  complex_field operator

@example
load_source <char[]>
@end example

The complex field is contained the specified file (SciDAC format).
The convolution takes place on the time slice of the underlying source.

@node complex_field_fm operator, evenandodd_wall operator, complex_field operator, Modified sources
@subsubsection complex_field_fm operator
@cindex complex_field_fm operator

@example
load_source <char[]>
@end example

Same as @strong{complex_field}, but the file is in FNAL format.

@node evenandodd_wall operator, gaussian operator, complex_field_fm operator, Modified sources
@subsubsection evenandodd_wall operator
@cindex evenandodd_wall operator

There are no additional parameters.  This convolution is equivalent to
a projection onto zero momentum.  In other words it replaces the
source value at each site on the underlying source time slice with the
time-slice sum of the underlying source.

@node gaussian operator, identity operator, evenandodd_wall operator, Modified sources
@subsubsection gaussian
@cindex gaussian operator

@example
r0 <float>
@end example

The convolution uses a Gaussian of width r0 (centered at the origin).
The convention for the width @strong{r0} is the same as for the
@strong{gaussian} base source.

@node identity operator, wavefunction operator, gaussian operator, Modified sources
@subsubsection identity operator
@cindex identity operator

There are no parameters.  This operator simply produces a copy.  There
should be no need to use it for sources, but it has a use as a sink
operator, as described below.

@node wavefunction operator, covariant_gaussian operator, identity operator, Modified sources
@subsubsection wavefunction operator
@cindex wavefunction operator

@example
wavefunction
load_source <char[]>
a <float>
@end example

This convolution uses the wave function in the specified ASCII file. As
with the analogous base source, the wave function is in physical (fm)
units.  It is converted to lattice units using lattice spacing @strong{a}.

@node covariant_gaussian operator, deriv1 operator, wavefunction operator,  Modified sources
@subsubsection covariant_gaussian operator
@cindex covariant_gaussian operator

@example
r0 <float>
source_iters <int = n>
@end example

This operator applies a covariant Gaussian with the specified width
and iterations. It is define by 
@tex $[1 + r_0^2 D^2/n]^2$
@end tex 
The discrete covariant Laplacian 
@tex $D^2$
@end tex 
is constructed from
the original unfattened gauge field.

@node deriv1 operator, deriv2_D operator, covariant_gaussian operator,  Modified sources
@subsubsection deriv1 operator
@cindex deriv1 operator

@example
dir <x|y|z>
disp <int = d>
weights <float[d]>
@end example

This operator applies a covariant finite difference in the direction
specified by @strong{dir}. The difference operation is specified by a
template over @strong{2d+1} lattice sites with the specified weights.
So to get the central finite difference, use disp 1 and weights 1.

@node deriv2_D operator, deriv2_B operator, deriv1 operator,  Modified sources
@subsubsection deriv2_D operator
@cindex deriv2_D operator

@example
dir <x|y|z> <x|y|z>
disp <int = d>
weights <fload[d]>
@end example

Computes a symmetric covariant second derivative based on the
specified template and directions.  That is for @strong{dir x y} the
operation results in @strong{D_x D_y + D_y D_x}, where @strong{D_i} is
the @strong{deriv1} operation above.

@node deriv2_B operator, deriv3_A operator, deriv2_D operator, Modified sources
@subsubsection deriv2_B operator
@cindex deriv2_B operator

@example
dir <x|y|z> <x|y|z>
disp <int = d>
weights <float[d]>
@end example

Computes an antisymmetric covariant second derivative based on the
specified template and directions.  That is, for @strong{dir x y} the
operation results in @strong{D_x D_y - D_y D_x}, where @strong{D_i} is
the @strong{deriv1} operation above.

@node deriv3_A operator, fat_covariant_gaussian operator, deriv2_B operator, Modified sources
@subsubsection deriv3_A operator
@cindex deriv3_A operator

@example
disp <int = d>
weights <float[d]>
@end example

This operation is not yet supported.

@node fat_covariant_gaussian operator, hop operator, deriv3_A operator,  Modified sources
@subsubsection fat_covariant_gaussian operator
@cindex fat_covariant_gaussian operator

@example
r0 <float>
source_iters <int>
@end example

This operation is the same as covariant_gaussian, except that the APE
smeared links are used.  See the gauge-field description above for
APE smearing.

@node hop operator, funnywall1 operator, deriv3_A operator,  Modified sources
@subsubsection hop operator
@cindex hop operator

@example
derivs <int>
dir <"sx"|"sy"|"sz"|"st">
eps_naik <float>
@end example

where ``s'' is ``+'', +-'', or empty. 
Apply the hopping matrix in the mu = x, y, z, or t direction or its <int>th
``derivative''.  That is, for <int> even, multiply by

   (1 + gamma_mu) U_x,mu \delta_x,x+mu + (1 - gamma_mu) U^\dagger_(x-mu,mu)

and for <int> odd, multiply by

   (1 + gamma_mu) U_x,mu \delta_x,x+mu - (1 - gamma_mu) U^\dagger_(x-mu,mu)

With <int> odd an s = ``+'' and ``-'' this operator can be used to
construct the x, y, z, or t component of the conserved current.  To do
that involves applying the + hop to the quark and the - hop to the
antiquark before tying them together.

With s = ``+'' apply only the forward hop (first term above).  With s
= ``-'' apply only the backward hop (second term above).  With s empty, apply both.

@node funnywall1 operator, funnywall2 operator, hop operator, Modified sources
@subsubsection funnywall1 operator
@cindex funnywall1 operator

No parameters.  This operation applies the staggered fermion
``funnywall1'' operator that couples to a subset of tastes.  It is
meaningful only for staggered fermion sources and propagators.

@node funnywall2 operator, ks_inverse operator, funnywall1 operator, Modified sources
@subsubsection funnywall2 operator
@cindex funnywall2 operator

Same as above, but for the ``funnywall2'' operator.

@node ks_inverse operator, spin_taste operator, funnywall2 operator, Modified sources
@subsubsection ks_inverse operator
@cindex ks_inverse operator

@example
ks_inverse
mass <float>
naik_term_epsilon <float>
u0 <float>
max_cg_iterations <int>
max_cg_restarts <int>
error_for_propagator <float>
rel_error_for_propagator <float>
precision <1|2>
coordinate_origin <int[4]>
momentum_twist <float[3]>
time_bc <antiperiodic|periodic>
@end example

Applies the inverse of the asqtad or HISQ Dirac operator.

@node spin_taste operator, ext_src_ks operator, ks_inverse operator, Modified sources
@subsubsection spin_taste operator
@cindex spin_taste operator

@example
spin_taste pion5
@end example

Applies staggered spin-taste offsets and phases

@node ext_src_ks operator, rotate_3D operator, spin_taste operator, Modified sources
@subsubsection ext_src_ks operator
@cindex ext_src_ks operator

@example
spin_taste_extend pion5
momentum <int[3]>
t0 <int>
@end example

Projects a propagator to the specified time slice and applies the
specified spin-taste operator and inserts the specified momentum.
Note that the spin-taste operator for extended sources differs from
the spin-taste operator for the source or sink of a two-point
function.  Use @strong{spin_taste_extend} here.  The difference is
that the extended operator does not insert an additional gamm5-gamma5 spin-taste
factor.

@node rotate_3D operator, dirac_inverse operator, ext_src_ks operator, Modified sources
@subsubsection rotate_3D operator
@cindex rotate_3D operator

@example
d1 <float>
@end example

This operator applies the FNAL 3D rotation to a Dirac source or sink.

@node dirac_inverse operator, gamma operator, rotate_3D operator, Modified sources
@subsubsection dirac_inverse operator
@cindex dirac_inverse operator

@example
kappa <float>
clov_c <float>
u0 <float>
max_cg_iterations <int>
max_cg_restarts <int>
error_for_propagator <float>
rel_error_for_propagator <float>
precision <1|2>
coordinate_origin <int[4]>
momentum_twist <float[3]>
time_bc <antiperiodic|periodic>
@end example

This operator applies the inverse of the clover Dirac operator.

@node gamma operator, ks_gamma operator, dirac_inverse operator, Modified sources
@subsubsection gamma operator
@cindex gamma operator

@example
gamma G5
@end example

This operator multiplies by the specified gamma matrix.

@node ks_gamma operator, ks_gamma_inv operator, gamma operator, Modified sources
@subsubsection ks_gamma operator
@cindex ks_gamma operator

Multiplies by the gamma matrix operator that reduces naive fermions to staggered fermions.

@node ks_gamma_inv operator, ext_src_dirac operator, ks_gamma operator, Modified sources
@subsubsection ks_gamma_inv operator
@cindex ks_gamma_inv operator

Multiplies by the inverse of the gamma matrix operator that reduces naive fermions to staggered fermions.

@node ext_src_dirac operator, project_t_slice operator, ks_gamma_inv operator, Modified sources
@subsubsection ext_src_dirac operator
@cindex ext_src_dirac operator

@example
gamma G5
momentum <int[3]>
t0 <int>
@end example

Projects a propagator to the specified time slice and applies the
specified gamma matrix operator and inserts the specified momentum.

@node project_t_slice operator, momentum operator, ext_src_dirac operator, Modified sources
@subsubsection project_t_slice operator
@cindex project_t_slice operator

@example
t0 <int>
@end example

Projects values on the specified time slice.

@node momentum operator, modulation operator, project_t_slice operator, Modified sources
@subsubsection momentum operator
@cindex momentum operator

@example
momentum <int[3]>
@end example

Multiplies by the plane-wave phase site-by-site.

@node modulation operator, Propagator description, momentum operator, Modified sources
@subsubsection modulation operator
@cindex modulation operator

@example
file <char[]>
@end example

Reads the specified file containing a complex field and multiplies the
values site-by-site.

@node Propagator description, Quark description,  Modified sources, clover_invert2, 
@subsection Propagator description
@cindex  Propagator description

Propagators are solutions to the inhomogeneous Dirac equation with
either base or modified sources.  These solutions can then be modified
at the sink to implement appropriate sink interpolating operators.
For simplicity we call the unmodified propagators ``propagators'' and
the modified propagators ``quarks''.  The ``quarks'' are used to
construct the hadron propagators.  Here we describe the
``propagators''.

The code generates three classes of propagators, namely clover, naive,
and ``extended naive''.  The naive propagator is constructed starting
from a (uually improved) staggered propagator, and multiplying by a
spin matrix that reverses the Kawamoto-Smit staggered spin rotation.
The extended naive propagator is singled out because the source
treatment is special.  It is intended for applications that generate a
naive propagator from the special extended Dirac sources produced by
the @strong{ext_src} application.  In that case four staggered
inversions are required for each Dirac source, one for each of the
four spin components.  The extended Dirac source on a single time
slice is usually the value on that time slice of a Dirac propagator
that started propagation from an underlying source on a distant time
slice. So there is one extended Dirac source for each spin and color
of the underlying source.  The number of staggered inversions is
therefore four times the number of underlying spins and colors.

One must pay particular attention to the spin structure in the
conversion from a staggered to a naive propagator.  The spin matrix in
the conversion depends on the source and sink 2^4 hypercube
coordinates.  The sink dependence is not problematic.  The source
dependence is, however.  The conversion fails when the source for the
staggered propagator has support on different hypercube sites.  Thus,
as a rule, the source for a naive propagator should be constructed
with the @strong{subset corner} mask, so it has support on only the
hypercube origins.  For the special case of an extended naive
propagator, the spin conversion is partially done in the
@strong{ext_src} application and completed in the
@strong{clover_invert2} application.  In this way the extended source
can have support on all spatial sites on the source time slice.  If
the extended source is not constructed by the @strong{ext_src}
application, it, too, should be restricted to the hypercube origins.

The propagators are specified in a manner similar to the sources.  The
number of propagators is specified, followed by that many stanzas, one
for each propagator.


@example
number_of_propagators <int p>

# propagator 0

[propagator_stanza]

# propagator 1

[propagator_stanza]

...

# propagator p-1

[propagator_stanza]
@end example

The @strong{[propagator_stanza]} has the following structure;

@example
[propagator_type]
[propagator_parameters]
check <yes|no>
error_for_propagator <float>
rel_error_for_propagator <float>
precision <1|2>
momentum_twist <float[3]>
time_bc <periodic|antiperiodic>
source <int>
[propagator input specification]
[propagator output specification]
@end example

There are three propagator types with their corresponding parameters:

@example
propagator_type clover
kappa <float>
clov_c <float>
@end example
These lines specify a clover propagator and its parameters.
The @strong{clov_c} factor multiplies a tadpole factor of 1/u0^3,
resulting in an effective clover coefficient of @strong{clov_c/u0^3}.

@example
propagator_type KS
mass <float>
# With naive HISQ quarks, we also have ...
naik_term_epsilon <float>
@end example

These lines specify a naive propagator with its mass.

@example
propagator_type KS4
mass <float>
@end example

These lines specify an extended naive propagator, starting from an
extended @strong{KS4}-type Dirac source.  (Please see the discussion
above about the spin conversion.)

The @strong{check} line specifies whether the code should calculate/
recalculate the propagator.  If the propagator is unknown, of course,
we would want to compute it from the specified source.  If the
propagator is being read from a file, we might still want to check it
by handing its source and supposed solution to the inverter and
recalculating it.  One or two CG iterations may be all that is needed
to check it.  In both cases we would say @strong{check yes}.  If the
propagator is being read from a file, and we prefer not to check it,
we would say @strong{check no}.  In that case, it doesn't matter what
source we associate it with, since the source would not be used.

Since currently a source is generated and, if requested, saved to a
file only if a propagator is built from it, in some cases we would
still like to create the source without evaluating the propagator.
Also, in some cases we want the propagator to be equal to the source
(as though the quark mass were infinite.)  In either case we would
write a dummy propagator stanza that calls for the source, but say
@strong{check sourceonly}.  The source is created, the inverter is
not called, and the propagator is set equal to the source (with zeros
away from the source).

There is one other subtlety with the current design.  With
@strong{check no}, the propagator is not calculated, but it is taken
from a file.  In that case, clearly, the source is unused and
irrelevant.  Even so, a source must be described and specified.  In
that case one should use the source type
@strong{vector_propagator_file} or @strong{dirac_propagator_file} and
specify the correct number of colors/sources in the file.

The @strong{error_for_propagator} specifies the desired upper bound
for the Cartesian norm ratio
@tex
$|resid|/|source|$
@end tex
where @strong{resid} is the residual vector.  Note, this is not the
squared upper bound.  The @strong{rel_error_for_propagator} specifies
the desired upper bound for the ``relative norm''
@tex
$\sqrt{\sum_x|resid(x)|^2/|soln(x)|^2/V}$
@end tex
where @strong{|resid(x)|} is the magnitude of the residual vector
(magniude of the Dirac spinor or color vector) on a single site
@strong{x}, @strong{|soln(x)|} is the magnitude of the solution vector
on a single site @strong{x} and @strong{V} is the lattice volume.  The
relative norm is preferred over the Cartesian norm for heavy quark
propagators that decrease sharply, since it is sensitive to the small
components.

The @strong{precision} line specifies whether the propagator is to be
computed in single or double precision.  This feature is supported in
if the code is compiled with QOPQDP package, but not otherwise.

The @strong{momentum_twist} injects momentum through the so-called
"boundary twist".  Three twist angles are specified for the x, y, and
z directions.  They are given in units of pi/L.  Thus a twist of 1
changes the standard periodic boundary condition in the corresponding
spatial direction to antiperiodic.

The @strong{time_bc} specifies whether to use periodic or antiperiodic
boundary conditions in the time direction.

The @strong{source} line specifies the source sequence number for the
source upon which the propagator is built.

For @strong{clover} and @strong{KS4} type propagators the
@strong{[propagator input specification]} has the syntax
@example
fresh_wprop |
continue_wprop |
reload_ascii_wprop <char[]> |
reload_serial_wprop <char[]> |
reload_parallel_wprop <char[]>
@end example
For the standard naive propagator (type @strong{KS}) the
@strong{[propagator input specification]} has the form
@example
fresh_ksprop |
continue_ksprop |
reload_ascii_ksprop <char[]> |
reload_serial_ksprop <char[]> |
reload_parallel_ksprop <char[]>
@end example
Both of these imitate the corresponding specifications for the gauge field.

Likewise, the output specification for @strong{clover} and
@strong{KS4} propagators has the form
@example
forget_wprop |
save_ascii_wprop <char[]> |
save_serial_fm_wprop <char[]> |
save_serial_fm_sc_wprop <char[]> |
save_serial_scidac_wprop <char[]> |
save_parallel_scidac_wprop <char[]> |
save_partfile_scidac_wprop <char[]> |
save_multifile_scidac_wprop <char[]> |
@end example

and for type @strong{KS} propagators,

@example
forget_wprop |
save_ascii_ksprop <char[]> |
save_serial_fm_ksprop <char[]> |
save_serial_fm_sc_ksprop <char[]> |
save_serial_scidac_ksprop <char[]> |
save_parallel_scidac_ksprop <char[]> |
save_partfile_scidac_ksprop <char[]> |
save_multifile_scidac_ksprop <char[]> |
@end example


@node Quark description, Combine description, Propagator description, clover_invert2, 
@subsection Quark description
@cindex  Quark description

In the terminology of the code ``quarks'' are ``propagators'' with a
sink treatment applied.  Only quarks can be combined to make hadrons,
so in case no sink treatment is desired, a trivial identity operator
is provided to convert a propagator into a ``quark''. .  Each quark is
constructed by operating (with one of the several operators) on a
previoiusly defined propagator or a previously defined quark.  In this
code hadrons are built from quarks, but not directly from
propagators. Propagators are referred to by their sequence number,
starting with zero.  Quarks are referred to by their sequence number,
also starting with zero.  In this respect the convention differs from
the convention for the base and modified sources.  The specification
for quarks is similar to that for propagators.

@example
number_of_quarks <int q>

# quark 0

[quark_stanza]

# quark 1

[quark_stanza]

...

# quark q-1

[quark_stanza]
@end example

The @strong{[quark_stanza]} has the form

@example
propagator|quark <int>
[operator_specification]
op_label <char[]>
[propagator_output]
@end example

The first line specifies either a @strong{propagator} index or a
@strong{quark} index.  The choices for the
@strong{[operator_specification]} are the same as for the modified
sources above, with the exception that the operators act on the
propagator/quark field on all time slices.  Other parameters have the
same function as above.


@node Combine description, Hadron description, Quark description, clover_invert2, 
@subsection Combine description
@cindex  Combine description

@example
combine <int = n>
quarks <int[n]>
coeffs <float[n]>
<propagator output description>
@end example

Creates a single quark from a linear combination of the listed quarks.
The coefficients give the weights of linear combination.

@node Hadron description, ks_spectrum, Quark description, clover_invert2, 
@subsection Hadron description
@cindex  Hadron description

Hadron correlators are formed by tying together the ``quarks'' at the
source and sink.  Correlators for both mesons and baryons (to some
extent) and a special ``open meson'' correlator are supported.  (The
open meson omits tying the sink spins and colors together.  Thus the
open meson correlator consists of a complex 12 by 12 matrix field.  It
is written in FermiQCD binary format for post processing.)

The hadron specification starts with the number of ``pairings'' of
quarks, followed by that many hadron stanzas:

@example
number_of_pairings <int h>

# pair 0

[hadron_stanza]

# pair 1

[hadron_stanza]

...

# pair h-1
@end example

[hadron_stanza]

A hadron stanza has the following structure

@example
pair <int> <int>
spectrum_request <[meson],[baryon]|open_meson>
save_corr_fnal <char[]>
r_offset <int[4]>
number_of_correlators <int>
<correlator_line>
<correlator_line>
...
<correlator_line>
@end example

The @strong{pair} line gives the sequence numbers of the ``quarks'' to
be used to form the mesons or baryons in this stanza.  In the case of
mesons the first quark index in the @strong{pair} line refers to the
antiquark and the second, to the quark.  In the case of baryons,
currently, the code supports only correlators in which at least two of
the three valence quarks are identical and form protons,
deltas, and lambdas.

The @strong{spectrum_request} line specifies the types of hadron
correlators to be computed.  The request is written as a
comma-separated string with no white space.  The code does not compute
both open meson and ``closed'' hadron propagators for a given pairing.

The @strong{save_corr_fnal} line specifies a file to which all
correlators in this pairing are written.  The format follows a style
introduced by the Fermilab Lattice collaboration.  Note that data is
always appended to the file.

The @strong{r_offset} causes the sink hadron locations (and
correlators) to be defined relative to the specified offset
coordinate.  If the source is specified at the same coordinate, the
result should be the same as translating the gauge field by minus this
coordinate and putting the source at the zero origin with zero offset.

The correlator lines are for mesons only.  They specify the source and
sink gamma matrices, phase factors, and momentum projections.  They
have the syntax

@example
correlator <char[] = label> <char[] = momlabel> 
   <1|-1|i|-i> <*|/> <float = norm> <Gsource> <Gsink> 
   <int[3] = mom> <char[3] = E|O|EO = reflection_parity>
@end example

Here is a specific example of nine related correlators:

@example
correlator RHO     p100  1 * 1 GX  GX  1 0 0 E E E
correlator RHO     p100  1 * 1 GX  GX  0 1 0 E E E
correlator RHO     p100  1 * 1 GX  GX  0 0 1 E E E
correlator RHO     p100  1 * 1 GY  GY  1 0 0 E E E
correlator RHO     p100  1 * 1 GY  GY  0 1 0 E E E
correlator RHO     p100  1 * 1 GY  GY  0 0 1 E E E
correlator RHO     p100  1 * 1 GZ  GZ  1 0 0 E E E
correlator RHO     p100  1 * 1 GZ  GZ  0 1 0 E E E
correlator RHO     p100  1 * 1 GZ  GZ  0 0 1 E E E
@end example

Here the correlator @strong{label} is RHO and @strong{momlabel} is
p100.  Correlators with the same combination of @strong{label} and
@strong{momlabel}, as in this example, are averaged.  The fourth field
specifies the phase that multiplies the correlator.  Choices are 1,
-1, i, and -i. The fifth and sixth fields specify a normalization
factor, which is multiplied or divided, as specified by the fifth
field.

The source and sink gamma matrices are denoted G1 G5 GX GY GZ GT GXY
GZX GYZ GXT GYT GZT G5X G5Y G5Z G5T, where G1 is the identity, and the
others should be obvious.  Note that it has been an unfortunate but
unbroken tradition in our code to use the wrong sign for GY, so in
cases where it matters, this can be corrected with an appropriate
phase factor.

The momentum px, py, pz is specified by three integers kx ky kz in
units of 2 pi/nx, 2 pi/ny, and 2 pi/nz.  The phase factor for the
momentum Fourier transform is usually exp(ip_j*r_j) for each direction
j, but we usually want to combine results with momentum p_j and -p_j.
Rather than having to enumerate all the signs, which would double the
number of lines in this example, and cause worse proliferation for
momentum 1 1 1, we simply specify whether the results for opposite
momentum components are to be added or subtracted.  With E we add
them, so the Fourier phase factor becomes  cos(p_j*r_j).  With O we
subtract, so it becomes sin(p_j*r_j).  With EO the exp(ip_j*r_j) is
kept and the correlator with the opposite momentum component is
ignored.  This reflection parity is specified for each of the three
directions in the order x, y, z.


@node ks_spectrum, Concept Index, Documentation for Specific Applications, Documentation for Specific Applications
@section ks_spectrum
@cindex ks_spectrum

This code generates staggered propagators from a wide variety of
sources and ties them together to form meson and baryon correlators.
Actions supported include asqtad and HISQ.  Like the clover_invert2
code, this code is designed to be flexible in specifying source
interpolating operators, propagators, sink interpolating operators,
and the combinations to form hadrons.  The design is similar to the
clover_invert2 code. In broad terms a hadron is contructed by tying
together quark and antiquark propagators.  These quark propagators are
generated in a succession of steps.  First a base source is defined.
A base source can then be modified by applying a sequence of source
operators to complete the definition of the source interpolating
operator.  The propagator starts at the source and propagates to
any sink location.  At the sink a variety of operators can be applied
to form the completed quark propagator.  They are then used to form
the hadrons.

Because of the high degree of flexibility in the code. the parameter
input file is highly structured.  We explain here the construction of
the parameter input file in some detail.  Examples can be found in the
directory @strong{ks_spectrum/test/*.sample-in}.

The parameter input file consists of the following required parts in
this order:

@menu
* KS geometry etc. specification::
* KS gauge field specification::
* KS quark condensate specification::
* KS base sources::
* KS modified sources::
* KS propagator set description::
* KS quark description::
* KS combine description::
* KS meson description::
* KS baryon description::
@end menu

This complete set of commands can be repeated indefinitely to form a
chain of consecutive computations.

For human readability, comments on lines beginning with hash (#) are
allowed in the parameter input file.  Blank lines are also permitted.

@sp 1

@node KS geometry etc. specification, KS gauge field specification, ks_spectrum, ks_spectrum
@subsection KS geometry etc. specification
@cindex KS geometry etc. specification

The geometry specification is the same as for the
@strong{clover_invert2} code.  @xref{Geometry etc. specification}.

@node KS gauge field specification, KS quark condensate specification, KS geometry etc. specification, ks_spectrum
@subsection KS gauge field specification
@cindex KS gauge field specification

The gauge field specification is the same as for the
@strong{clover_invert2} code.  @xref{Gauge field specification}.

@node KS quark condensate specification, KS base sources, KS gauge field specification, ks_spectrum
@subsection KS quark condensate specification
@cindex KS quark condensate specification

Provision is made for calculating the chiral condensate for any number
of masses.  This input section starts by specifying the number of such
masses.  If no chiral condensates are to be computed, this number
should be zero and no other input lines are required.

@example
number_of_pbp_masses <int = n>

max_cg_iterations <int>
max_cg_restarts <int>
npbp_reps <int>
prec_pbp <1|2>

# mass 0

mass <float>
error_for_propagator <float>
rel_error_for_propagator <float>

# mass 1

mass <float>
error_for_propagator <float>
rel_error_for_propagator <float>

etc.

# mass n-1

mass <float>
error_for_propagator <float>
rel_error_for_propagator <float>
@end example

The @strong{max_cg_iterations} and @strong{max_cg_restarts} control
the conjugate gradient iterations.  The overall maximum number of
iterations is the product of thse two.

The chiral condensate is calculated using a stochastic estimator.  The
number of such estimators is specified by the @strong{npbp_reps} line.
The precision of the conjugate gradient calculation is specified by
@strong{prec_pbp}.  This request has effect only if the code is built
with the SciDAC QOP package.

The remaining stanzas specify the masses and the stopping condition
for the conjugate gradient. The conventions for residual errors are
explained in the @strong{clover_invert2} propagator
specification. @xref{Propagator description}.

With HISQ actions it is also necessary to specify the Naik term
epsilon parameter for each mass, so an additional line is required in
each mass stanza:

@example
mass <float>
naik_term_epsilon <float>
error_for_propagator <float>
rel_error_for_propagator <float>
@end example

@node KS base sources, KS modified sources, KS quark condensate specification, ks_spectrum
@subsection KS base sources
@cindex KS base sources

The base sources are specified in the same manner as for the
@strong{clover_invert2} code, except that Dirac sources are not
appropriate. They are not repeated here. @xref{Base sources}.

@node KS modified sources, KS propagator set description, KS base sources, ks_spectrum
@subsection KS modified sources
@cindex KS modified sources

The base sources are modified in the same was as for the
@strong{clover_invert2} code.  The description is not repeated
here. @xref{Modified sources}.  As before, the numbering is
sequential, starting with the base sources and continuing with the
modified sources.

@node KS propagator set description, KS quark description, KS modified sources, ks_spectrum
@subsection KS propagator set description
@cindex KS propagator set description

As with the @strong{clover_invert} code, we first define
``propagators'' that start from either base sources or modified
sources.  A series of sink operators then act on propagators,
resulting in ``quarks''.  They, in turn are tied together to form
mesons and baryons.  Unlike the @strong{clover_invert} code, the
staggered fermion propagators are defined in ``sets'' that share a
common source and differ only in their masses.  This organization
allows more efficient use of the multimass inverters.  Sets are
defined with the following series of input lines:

@example
number_of_sets <int = s>

# set 0

[set_stanza]

# set 1

[set_stanza]

etc.

# set s-1

[set_stanza]
@end example

The @strong{[set_stanza]} begins with parameters common to all
memberso of the set:

@example
max_cg_iterations <int>
max_cg_restarts <int>
check <yes|no>
momentum_twist <float[3]>
time_bc <periodic|antiperiodic>
source <int>
@end example

The above parameters have the same meaning as the corresponding
parameters for the @strong{clover_invert2} code. @xref{Propagator
description}.

The @strong{[set_stanza]} continues with a list of parameters for each
propagator in the set.

@example
number_of_propagators <int=p>

# propagator 0

[propagator_stanza]

# propagator 1

[propagator_stanza]

etc.

# propagator p-1

[propagator_stanza]
@end example

Propagators are numbered sequentially starting with zero and
continuing into the next set.  So after the first set with @strong{p}
propagators, the first propagator in the second set is number
@strong{p}.

The propagator stanza is simply

@example
mass <float>
# If HISQ, we also have ...
naik_term_epsilon <float>

error_for_propagator <float>
rel_error_for_propagator <float>

# Propagator input specification
fresh_ksprop |
continue_ksprop |
reload_ascii_ksprop <char[]> |
reload_serial_ksprop <char[]> |
reload_parallel_ksprop <char[]>

# Propagator output specification
forget_ksprop |
save_ascii_ksprop <char[]> |
save_serial_scidac_ksprop <char[]> |
save_parallel_scidac_ksprop <char[]> |
save_partfile_scidac_ksprop <char[]> |
save_multifile_scidac_ksprop <char[]> |
@end example

@node KS quark description, KS combine description, KS propagator set description, ks_spectrum
@subsection KS quark description
@cindex KS quark description

The ``quarks'' are constructed exactly as with the
@strong{clover_invert2} code, so the description is not repeated
here. @xref{Quark description}.


@node KS combine description, KS meson description, KS quark description, ks_spectrum
@subsection Combine description
@cindex  Combine description

@example
combine <int = n>
quarks <int[n]>
coeffs <float[n]>
<propagator output specification>
@end example

@node KS meson description, KS baryon description, KS quark description, ks_spectrum
@subsection KS meson description
@cindex KS meson description

The meson and baryon specification is very similar to the
specification for the @strong{clover_invert2} code.

@example
number_of_mesons <int m>

# pair 0

[meson_stanza]

# pair 1

[meson_stanza]

...

# pair h-1

[meson_stanza]
@end example

The @strong{[meson_stanza]} follows the same pattern as for the
@strong{clover_invert} code:.  @xref{Hadron description}.

@example
pair <int> <int>
spectrum_request meson
save_corr_fnal <char[]>
r_offset <int[4]>
@end example

The only @strong{spectrum_request} choice is ``meson'' at the moment.
Baryons are specified separately later in this code.  The correlator
output and offset have the same meaning as with the
@strong{clover_invert2} code.

The @strong{[meson_stanza]} concludes with a specification of the
meson correlators:

@example
number_of_correlators <int>
[correlator_line]
[correlator_line]
...
[correlator_line]
@end example

The @strong{[correlator_line]} has the following format:

@example
correlator <char[] = label> <char[] = momlabel> <1|-1|i|-i> [*/] <float = norm> 
   <spin_taste_sink> <int[3] = mom> <char[3] = E|O|EO = reflection_parity>
@end example

As with the @strong{clover_invert2} correlator specification,
correlators with the same label/momlabel are averaged.  The correlator
is multiplied by the specified phase and normalization factor.

Unlike Dirac quarks, with staggered quarks the source spin-taste
content is built into the source and propagator and cannot be modified
once the propagator is generated.  So only the sink spin-taste
combination is specified in the @strong{correlator} stanza.  To obtain
a variety of sink spin-taste combinations from the same pair of
quarks, it is necessary to use a broad-spectrum source, such as one of
the wall sources.

Possible sink spin-taste combinations are these:

@example
pion5,  pion05,  pioni5,  pionij,  pioni,  pioni0,  pions,  pion0,  rhoi,
rhox,  rhoy,  rhoz,  rhoi0,  rhox0,  rhoy0,  rhoz0,  rhoxs,  rhoys,  rhozs,
rhots,  rhois,  rho0,  rhoxsfn,  rhoysfn,  rhozsfn,  rhotsfn,  rhoisfn
@end example

For those familiar with it, the labeling follows our conventions in
the older @strong{spectrum_nlpi2} code, except that rhox, rhoy, and
rhoz = rhoi, rhoxs, rhoys, rhozs = rhois, rhots, and the "fn" choices
are new.

Normally the operator uses a symmetric shift, but the code can be
compiled with -DONE_SIDED_SHIFT to get a one-sided shift

The "fn" choices use the fat and long links in place of the thin links
for parallel transport, so the vector operator is conserved.  However,
in the current code "fn" is always one-sided and uses only the asqtad
fat links -- not the long links.

The momentum is specified in the same manner as for the
@strong{clover_invert2} code.

@node KS baryon description, Concept Index, KS meson description, ks_spectrum
@subsection KS baryon description
@cindex KS baryon description

In the @strong{ks_spectrum} code baryons are specified separately from
mesons.  

@example
number_of_baryons <int b>

# triplet 0

[baryon_stanza]

# pair 1

[baryon_stanza]

...

# pair b-1

[baryon_stanza]
@end example

The @strong{[baryon_stanza]} has the following format in analogy with
the @strong{[meson_stanza]}:

@example
triplet <int> <int> <int>
spectrum_request baryon
save_corr_fnal <char[]>
r_offset <int[4]>
@end example

and continues with a specification of the correlators

@example
number_of_correlators <int>
[correlator_line]
[correlator_line]
...
[correlator_line]
@end example

The correlator line has the format

@example
correlator <char[] = label> <char[] = momlabel> <1|-1|i|-i> [*/] <float = norm> 
   <sink> 
@end example

The only sink choices are "nucleon" and "delta".

Currently the code does not support a momentum insertion at the sink.

@node Concept Index, Variable Index , Writing Your Own Application, Top
@unnumbered Concept Index
@printindex cp
@page

@node Variable Index , , Concept Index, Top
@unnumbered Variable Index
@printindex vr
@page
@contents

@bye


